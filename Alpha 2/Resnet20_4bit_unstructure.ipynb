{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "radical-fifty",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "ResNet_Cifar(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter   \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"resnet20_quant\"\n",
    "model = resnet20_quant()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "junior-reminder",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.190 (0.190)\tData 0.127 (0.127)\tLoss 2.5060 (2.5060)\tPrec 7.812% (7.812%)\n",
      "Epoch: [0][100/391]\tTime 0.052 (0.048)\tData 0.002 (0.003)\tLoss 1.9115 (1.9647)\tPrec 27.344% (25.557%)\n",
      "Epoch: [0][200/391]\tTime 0.046 (0.050)\tData 0.002 (0.003)\tLoss 1.4445 (1.8197)\tPrec 46.094% (31.374%)\n",
      "Epoch: [0][300/391]\tTime 0.040 (0.049)\tData 0.001 (0.002)\tLoss 1.4971 (1.7097)\tPrec 46.094% (35.673%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.119 (0.119)\tLoss 1.5237 (1.5237)\tPrec 50.000% (50.000%)\n",
      " * Prec 47.070% \n",
      "best acc: 47.070000\n",
      "Epoch: [1][0/391]\tTime 0.163 (0.163)\tData 0.117 (0.117)\tLoss 1.3897 (1.3897)\tPrec 49.219% (49.219%)\n",
      "Epoch: [1][100/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 1.3328 (1.2351)\tPrec 58.594% (54.858%)\n",
      "Epoch: [1][200/391]\tTime 0.044 (0.046)\tData 0.002 (0.002)\tLoss 1.0978 (1.1678)\tPrec 62.500% (57.696%)\n",
      "Epoch: [1][300/391]\tTime 0.043 (0.045)\tData 0.002 (0.002)\tLoss 0.9447 (1.1299)\tPrec 67.188% (59.235%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.125 (0.125)\tLoss 1.0587 (1.0587)\tPrec 60.156% (60.156%)\n",
      " * Prec 60.850% \n",
      "best acc: 60.850000\n",
      "Epoch: [2][0/391]\tTime 0.177 (0.177)\tData 0.132 (0.132)\tLoss 0.8788 (0.8788)\tPrec 71.875% (71.875%)\n",
      "Epoch: [2][100/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.9779 (0.9858)\tPrec 67.188% (64.658%)\n",
      "Epoch: [2][200/391]\tTime 0.051 (0.044)\tData 0.002 (0.002)\tLoss 0.8778 (0.9479)\tPrec 64.844% (65.940%)\n",
      "Epoch: [2][300/391]\tTime 0.042 (0.044)\tData 0.002 (0.002)\tLoss 0.9006 (0.9294)\tPrec 64.062% (66.725%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 1.5056 (1.5056)\tPrec 51.562% (51.562%)\n",
      " * Prec 54.710% \n",
      "best acc: 60.850000\n",
      "Epoch: [3][0/391]\tTime 0.199 (0.199)\tData 0.147 (0.147)\tLoss 0.9371 (0.9371)\tPrec 67.969% (67.969%)\n",
      "Epoch: [3][100/391]\tTime 0.050 (0.048)\tData 0.002 (0.003)\tLoss 0.7523 (0.8155)\tPrec 77.344% (71.380%)\n",
      "Epoch: [3][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.8508 (0.8168)\tPrec 71.875% (71.401%)\n",
      "Epoch: [3][300/391]\tTime 0.040 (0.046)\tData 0.002 (0.003)\tLoss 0.8335 (0.8019)\tPrec 73.438% (71.787%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.126 (0.126)\tLoss 0.7803 (0.7803)\tPrec 71.875% (71.875%)\n",
      " * Prec 69.860% \n",
      "best acc: 69.860000\n",
      "Epoch: [4][0/391]\tTime 0.175 (0.175)\tData 0.131 (0.131)\tLoss 0.6901 (0.6901)\tPrec 75.000% (75.000%)\n",
      "Epoch: [4][100/391]\tTime 0.043 (0.046)\tData 0.002 (0.003)\tLoss 0.7575 (0.7251)\tPrec 74.219% (74.714%)\n",
      "Epoch: [4][200/391]\tTime 0.047 (0.046)\tData 0.002 (0.003)\tLoss 0.9073 (0.7214)\tPrec 69.531% (74.992%)\n",
      "Epoch: [4][300/391]\tTime 0.045 (0.045)\tData 0.002 (0.002)\tLoss 0.6744 (0.7156)\tPrec 75.781% (75.075%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.123 (0.123)\tLoss 0.5846 (0.5846)\tPrec 77.344% (77.344%)\n",
      " * Prec 73.120% \n",
      "best acc: 73.120000\n",
      "Epoch: [5][0/391]\tTime 0.201 (0.201)\tData 0.141 (0.141)\tLoss 0.7876 (0.7876)\tPrec 74.219% (74.219%)\n",
      "Epoch: [5][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.003)\tLoss 0.7818 (0.6572)\tPrec 71.875% (77.119%)\n",
      "Epoch: [5][200/391]\tTime 0.041 (0.045)\tData 0.002 (0.003)\tLoss 0.5624 (0.6559)\tPrec 78.906% (77.099%)\n",
      "Epoch: [5][300/391]\tTime 0.042 (0.044)\tData 0.002 (0.002)\tLoss 0.7083 (0.6534)\tPrec 75.781% (77.287%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.5511 (0.5511)\tPrec 81.250% (81.250%)\n",
      " * Prec 76.490% \n",
      "best acc: 76.490000\n",
      "Epoch: [6][0/391]\tTime 0.183 (0.183)\tData 0.135 (0.135)\tLoss 0.5563 (0.5563)\tPrec 82.812% (82.812%)\n",
      "Epoch: [6][100/391]\tTime 0.051 (0.044)\tData 0.001 (0.003)\tLoss 0.5608 (0.6223)\tPrec 80.469% (78.527%)\n",
      "Epoch: [6][200/391]\tTime 0.041 (0.044)\tData 0.002 (0.002)\tLoss 0.6342 (0.6127)\tPrec 78.125% (78.759%)\n",
      "Epoch: [6][300/391]\tTime 0.038 (0.043)\tData 0.001 (0.002)\tLoss 0.5199 (0.6132)\tPrec 83.594% (78.667%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.112 (0.112)\tLoss 0.5418 (0.5418)\tPrec 82.812% (82.812%)\n",
      " * Prec 77.640% \n",
      "best acc: 77.640000\n",
      "Epoch: [7][0/391]\tTime 0.165 (0.165)\tData 0.123 (0.123)\tLoss 0.5735 (0.5735)\tPrec 84.375% (84.375%)\n",
      "Epoch: [7][100/391]\tTime 0.042 (0.043)\tData 0.001 (0.003)\tLoss 0.5631 (0.5658)\tPrec 81.250% (80.709%)\n",
      "Epoch: [7][200/391]\tTime 0.043 (0.042)\tData 0.001 (0.002)\tLoss 0.5740 (0.5685)\tPrec 75.781% (80.255%)\n",
      "Epoch: [7][300/391]\tTime 0.038 (0.042)\tData 0.001 (0.002)\tLoss 0.6314 (0.5706)\tPrec 77.344% (80.175%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.111 (0.111)\tLoss 0.7781 (0.7781)\tPrec 72.656% (72.656%)\n",
      " * Prec 73.270% \n",
      "best acc: 77.640000\n",
      "Epoch: [8][0/391]\tTime 0.165 (0.165)\tData 0.123 (0.123)\tLoss 0.5779 (0.5779)\tPrec 79.688% (79.688%)\n",
      "Epoch: [8][100/391]\tTime 0.039 (0.043)\tData 0.002 (0.003)\tLoss 0.6901 (0.5399)\tPrec 77.344% (80.995%)\n",
      "Epoch: [8][200/391]\tTime 0.050 (0.042)\tData 0.002 (0.002)\tLoss 0.5001 (0.5457)\tPrec 85.156% (80.997%)\n",
      "Epoch: [8][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.002)\tLoss 0.4658 (0.5419)\tPrec 85.938% (81.105%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 0.4480 (0.4480)\tPrec 85.156% (85.156%)\n",
      " * Prec 79.780% \n",
      "best acc: 79.780000\n",
      "Epoch: [9][0/391]\tTime 0.185 (0.185)\tData 0.132 (0.132)\tLoss 0.4446 (0.4446)\tPrec 83.594% (83.594%)\n",
      "Epoch: [9][100/391]\tTime 0.047 (0.045)\tData 0.002 (0.003)\tLoss 0.6804 (0.5131)\tPrec 75.000% (82.024%)\n",
      "Epoch: [9][200/391]\tTime 0.043 (0.045)\tData 0.002 (0.002)\tLoss 0.4575 (0.5144)\tPrec 85.938% (82.109%)\n",
      "Epoch: [9][300/391]\tTime 0.041 (0.045)\tData 0.002 (0.002)\tLoss 0.4165 (0.5114)\tPrec 88.281% (82.319%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.125 (0.125)\tLoss 0.4331 (0.4331)\tPrec 87.500% (87.500%)\n",
      " * Prec 80.610% \n",
      "best acc: 80.610000\n",
      "Epoch: [10][0/391]\tTime 0.193 (0.193)\tData 0.126 (0.126)\tLoss 0.3078 (0.3078)\tPrec 89.844% (89.844%)\n",
      "Epoch: [10][100/391]\tTime 0.042 (0.046)\tData 0.002 (0.003)\tLoss 0.4320 (0.4903)\tPrec 82.812% (83.075%)\n",
      "Epoch: [10][200/391]\tTime 0.045 (0.045)\tData 0.002 (0.003)\tLoss 0.5135 (0.4930)\tPrec 84.375% (82.797%)\n",
      "Epoch: [10][300/391]\tTime 0.041 (0.044)\tData 0.002 (0.002)\tLoss 0.5903 (0.4940)\tPrec 76.562% (82.633%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 0.5383 (0.5383)\tPrec 84.375% (84.375%)\n",
      " * Prec 80.480% \n",
      "best acc: 80.610000\n",
      "Epoch: [11][0/391]\tTime 0.178 (0.178)\tData 0.130 (0.130)\tLoss 0.5525 (0.5525)\tPrec 80.469% (80.469%)\n",
      "Epoch: [11][100/391]\tTime 0.043 (0.047)\tData 0.002 (0.003)\tLoss 0.5674 (0.4844)\tPrec 77.344% (83.323%)\n",
      "Epoch: [11][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.4535 (0.4749)\tPrec 84.375% (83.640%)\n",
      "Epoch: [11][300/391]\tTime 0.043 (0.044)\tData 0.002 (0.002)\tLoss 0.3986 (0.4670)\tPrec 87.500% (83.890%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.134 (0.134)\tLoss 0.4581 (0.4581)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.260% \n",
      "best acc: 81.260000\n",
      "Epoch: [12][0/391]\tTime 0.178 (0.178)\tData 0.131 (0.131)\tLoss 0.4377 (0.4377)\tPrec 83.594% (83.594%)\n",
      "Epoch: [12][100/391]\tTime 0.047 (0.045)\tData 0.002 (0.003)\tLoss 0.3352 (0.4570)\tPrec 88.281% (84.437%)\n",
      "Epoch: [12][200/391]\tTime 0.040 (0.044)\tData 0.001 (0.002)\tLoss 0.3853 (0.4608)\tPrec 86.719% (84.068%)\n",
      "Epoch: [12][300/391]\tTime 0.051 (0.043)\tData 0.002 (0.002)\tLoss 0.4250 (0.4617)\tPrec 82.812% (84.079%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.4839 (0.4839)\tPrec 79.688% (79.688%)\n",
      " * Prec 79.310% \n",
      "best acc: 81.260000\n",
      "Epoch: [13][0/391]\tTime 0.162 (0.162)\tData 0.121 (0.121)\tLoss 0.3837 (0.3837)\tPrec 87.500% (87.500%)\n",
      "Epoch: [13][100/391]\tTime 0.036 (0.042)\tData 0.002 (0.003)\tLoss 0.4460 (0.4388)\tPrec 84.375% (84.708%)\n",
      "Epoch: [13][200/391]\tTime 0.049 (0.042)\tData 0.002 (0.002)\tLoss 0.3705 (0.4352)\tPrec 88.281% (84.861%)\n",
      "Epoch: [13][300/391]\tTime 0.040 (0.042)\tData 0.001 (0.002)\tLoss 0.3550 (0.4376)\tPrec 89.844% (84.845%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 0.4712 (0.4712)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.270% \n",
      "best acc: 83.270000\n",
      "Epoch: [14][0/391]\tTime 0.166 (0.166)\tData 0.122 (0.122)\tLoss 0.4081 (0.4081)\tPrec 87.500% (87.500%)\n",
      "Epoch: [14][100/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.4892 (0.4374)\tPrec 83.594% (84.762%)\n",
      "Epoch: [14][200/391]\tTime 0.043 (0.040)\tData 0.001 (0.002)\tLoss 0.3361 (0.4283)\tPrec 89.062% (85.113%)\n",
      "Epoch: [14][300/391]\tTime 0.037 (0.040)\tData 0.001 (0.002)\tLoss 0.3767 (0.4263)\tPrec 90.625% (85.302%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.3560 (0.3560)\tPrec 88.281% (88.281%)\n",
      " * Prec 82.540% \n",
      "best acc: 83.270000\n",
      "Epoch: [15][0/391]\tTime 0.169 (0.169)\tData 0.128 (0.128)\tLoss 0.4148 (0.4148)\tPrec 86.719% (86.719%)\n",
      "Epoch: [15][100/391]\tTime 0.040 (0.043)\tData 0.001 (0.003)\tLoss 0.3131 (0.3983)\tPrec 89.062% (86.146%)\n",
      "Epoch: [15][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.002)\tLoss 0.4336 (0.4099)\tPrec 88.281% (85.906%)\n",
      "Epoch: [15][300/391]\tTime 0.041 (0.042)\tData 0.004 (0.002)\tLoss 0.2641 (0.4126)\tPrec 90.625% (85.743%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 0.4942 (0.4942)\tPrec 82.031% (82.031%)\n",
      " * Prec 82.090% \n",
      "best acc: 83.270000\n",
      "Epoch: [16][0/391]\tTime 0.167 (0.167)\tData 0.120 (0.120)\tLoss 0.3911 (0.3911)\tPrec 85.156% (85.156%)\n",
      "Epoch: [16][100/391]\tTime 0.041 (0.043)\tData 0.001 (0.003)\tLoss 0.3736 (0.3998)\tPrec 86.719% (86.154%)\n",
      "Epoch: [16][200/391]\tTime 0.041 (0.044)\tData 0.002 (0.002)\tLoss 0.4563 (0.4036)\tPrec 83.594% (86.097%)\n",
      "Epoch: [16][300/391]\tTime 0.037 (0.043)\tData 0.001 (0.002)\tLoss 0.3069 (0.4047)\tPrec 89.844% (85.984%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.111 (0.111)\tLoss 0.5917 (0.5917)\tPrec 75.000% (75.000%)\n",
      " * Prec 77.600% \n",
      "best acc: 83.270000\n",
      "Epoch: [17][0/391]\tTime 0.164 (0.164)\tData 0.119 (0.119)\tLoss 0.3588 (0.3588)\tPrec 87.500% (87.500%)\n",
      "Epoch: [17][100/391]\tTime 0.038 (0.045)\tData 0.002 (0.003)\tLoss 0.4805 (0.3786)\tPrec 88.281% (86.804%)\n",
      "Epoch: [17][200/391]\tTime 0.042 (0.044)\tData 0.002 (0.002)\tLoss 0.3725 (0.3840)\tPrec 85.938% (86.458%)\n",
      "Epoch: [17][300/391]\tTime 0.040 (0.044)\tData 0.002 (0.002)\tLoss 0.4168 (0.3882)\tPrec 82.031% (86.361%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.112 (0.112)\tLoss 0.3932 (0.3932)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.050% \n",
      "best acc: 84.050000\n",
      "Epoch: [18][0/391]\tTime 0.171 (0.171)\tData 0.130 (0.130)\tLoss 0.4412 (0.4412)\tPrec 82.812% (82.812%)\n",
      "Epoch: [18][100/391]\tTime 0.040 (0.041)\tData 0.001 (0.003)\tLoss 0.3193 (0.3695)\tPrec 89.844% (87.330%)\n",
      "Epoch: [18][200/391]\tTime 0.048 (0.041)\tData 0.002 (0.002)\tLoss 0.4128 (0.3786)\tPrec 86.719% (86.925%)\n",
      "Epoch: [18][300/391]\tTime 0.043 (0.041)\tData 0.002 (0.002)\tLoss 0.5137 (0.3819)\tPrec 80.469% (86.755%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.123 (0.123)\tLoss 0.3582 (0.3582)\tPrec 87.500% (87.500%)\n",
      " * Prec 82.670% \n",
      "best acc: 84.050000\n",
      "Epoch: [19][0/391]\tTime 0.164 (0.164)\tData 0.118 (0.118)\tLoss 0.2894 (0.2894)\tPrec 91.406% (91.406%)\n",
      "Epoch: [19][100/391]\tTime 0.040 (0.042)\tData 0.001 (0.003)\tLoss 0.2814 (0.3593)\tPrec 90.625% (87.461%)\n",
      "Epoch: [19][200/391]\tTime 0.044 (0.041)\tData 0.001 (0.002)\tLoss 0.3336 (0.3571)\tPrec 90.625% (87.461%)\n",
      "Epoch: [19][300/391]\tTime 0.044 (0.042)\tData 0.002 (0.002)\tLoss 0.3500 (0.3658)\tPrec 85.938% (87.248%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.3638 (0.3638)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.930% \n",
      "best acc: 84.050000\n",
      "Epoch: [20][0/391]\tTime 0.155 (0.155)\tData 0.115 (0.115)\tLoss 0.5002 (0.5002)\tPrec 82.031% (82.031%)\n",
      "Epoch: [20][100/391]\tTime 0.041 (0.046)\tData 0.001 (0.003)\tLoss 0.3308 (0.3497)\tPrec 90.625% (87.786%)\n",
      "Epoch: [20][200/391]\tTime 0.047 (0.044)\tData 0.002 (0.002)\tLoss 0.4045 (0.3568)\tPrec 85.156% (87.469%)\n",
      "Epoch: [20][300/391]\tTime 0.039 (0.044)\tData 0.001 (0.002)\tLoss 0.4046 (0.3529)\tPrec 85.156% (87.666%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.111 (0.111)\tLoss 0.4037 (0.4037)\tPrec 87.500% (87.500%)\n",
      " * Prec 82.770% \n",
      "best acc: 84.050000\n",
      "Epoch: [21][0/391]\tTime 0.177 (0.177)\tData 0.137 (0.137)\tLoss 0.2969 (0.2969)\tPrec 89.062% (89.062%)\n",
      "Epoch: [21][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.003)\tLoss 0.3741 (0.3560)\tPrec 84.375% (87.485%)\n",
      "Epoch: [21][200/391]\tTime 0.040 (0.044)\tData 0.001 (0.002)\tLoss 0.2701 (0.3544)\tPrec 90.625% (87.593%)\n",
      "Epoch: [21][300/391]\tTime 0.038 (0.043)\tData 0.001 (0.002)\tLoss 0.3126 (0.3542)\tPrec 90.625% (87.542%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.123 (0.123)\tLoss 0.3209 (0.3209)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.550% \n",
      "best acc: 84.550000\n",
      "Epoch: [22][0/391]\tTime 0.163 (0.163)\tData 0.117 (0.117)\tLoss 0.3935 (0.3935)\tPrec 80.469% (80.469%)\n",
      "Epoch: [22][100/391]\tTime 0.045 (0.045)\tData 0.002 (0.003)\tLoss 0.2626 (0.3410)\tPrec 91.406% (88.127%)\n",
      "Epoch: [22][200/391]\tTime 0.043 (0.045)\tData 0.002 (0.002)\tLoss 0.2514 (0.3425)\tPrec 88.281% (88.083%)\n",
      "Epoch: [22][300/391]\tTime 0.048 (0.046)\tData 0.002 (0.002)\tLoss 0.3873 (0.3463)\tPrec 85.938% (87.900%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.4458 (0.4458)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.000% \n",
      "best acc: 84.550000\n",
      "Epoch: [23][0/391]\tTime 0.178 (0.178)\tData 0.128 (0.128)\tLoss 0.2767 (0.2767)\tPrec 91.406% (91.406%)\n",
      "Epoch: [23][100/391]\tTime 0.044 (0.046)\tData 0.002 (0.003)\tLoss 0.3487 (0.3370)\tPrec 89.062% (88.142%)\n",
      "Epoch: [23][200/391]\tTime 0.047 (0.046)\tData 0.002 (0.002)\tLoss 0.3131 (0.3319)\tPrec 89.062% (88.270%)\n",
      "Epoch: [23][300/391]\tTime 0.040 (0.045)\tData 0.002 (0.002)\tLoss 0.3544 (0.3360)\tPrec 86.719% (88.310%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.127 (0.127)\tLoss 0.3878 (0.3878)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.560% \n",
      "best acc: 84.550000\n",
      "Epoch: [24][0/391]\tTime 0.174 (0.174)\tData 0.126 (0.126)\tLoss 0.2987 (0.2987)\tPrec 89.062% (89.062%)\n",
      "Epoch: [24][100/391]\tTime 0.045 (0.045)\tData 0.002 (0.003)\tLoss 0.3072 (0.3283)\tPrec 89.062% (88.645%)\n",
      "Epoch: [24][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.3806 (0.3316)\tPrec 85.156% (88.507%)\n",
      "Epoch: [24][300/391]\tTime 0.041 (0.044)\tData 0.002 (0.002)\tLoss 0.2067 (0.3310)\tPrec 92.969% (88.525%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.118 (0.118)\tLoss 0.2985 (0.2985)\tPrec 91.406% (91.406%)\n",
      " * Prec 85.510% \n",
      "best acc: 85.510000\n",
      "Epoch: [25][0/391]\tTime 0.168 (0.168)\tData 0.122 (0.122)\tLoss 0.2472 (0.2472)\tPrec 91.406% (91.406%)\n",
      "Epoch: [25][100/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.2554 (0.3137)\tPrec 88.281% (88.861%)\n",
      "Epoch: [25][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.3270 (0.3135)\tPrec 85.156% (88.856%)\n",
      "Epoch: [25][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.4069 (0.3196)\tPrec 85.156% (88.650%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 0.3216 (0.3216)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.160% \n",
      "best acc: 85.510000\n",
      "Epoch: [26][0/391]\tTime 0.169 (0.169)\tData 0.125 (0.125)\tLoss 0.3248 (0.3248)\tPrec 88.281% (88.281%)\n",
      "Epoch: [26][100/391]\tTime 0.043 (0.045)\tData 0.002 (0.003)\tLoss 0.3643 (0.2993)\tPrec 86.719% (89.472%)\n",
      "Epoch: [26][200/391]\tTime 0.043 (0.043)\tData 0.001 (0.002)\tLoss 0.2440 (0.3121)\tPrec 93.750% (89.039%)\n",
      "Epoch: [26][300/391]\tTime 0.041 (0.043)\tData 0.002 (0.002)\tLoss 0.2897 (0.3129)\tPrec 91.406% (88.977%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 0.4339 (0.4339)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.330% \n",
      "best acc: 85.510000\n",
      "Epoch: [27][0/391]\tTime 0.159 (0.159)\tData 0.114 (0.114)\tLoss 0.1621 (0.1621)\tPrec 96.094% (96.094%)\n",
      "Epoch: [27][100/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.3742 (0.2986)\tPrec 85.938% (89.534%)\n",
      "Epoch: [27][200/391]\tTime 0.039 (0.045)\tData 0.002 (0.002)\tLoss 0.3334 (0.3009)\tPrec 91.406% (89.381%)\n",
      "Epoch: [27][300/391]\tTime 0.044 (0.044)\tData 0.002 (0.002)\tLoss 0.3609 (0.3048)\tPrec 89.062% (89.358%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.4169 (0.4169)\tPrec 82.812% (82.812%)\n",
      " * Prec 84.240% \n",
      "best acc: 85.510000\n",
      "Epoch: [28][0/391]\tTime 0.160 (0.160)\tData 0.119 (0.119)\tLoss 0.3031 (0.3031)\tPrec 89.062% (89.062%)\n",
      "Epoch: [28][100/391]\tTime 0.038 (0.044)\tData 0.002 (0.003)\tLoss 0.2897 (0.3066)\tPrec 89.062% (89.163%)\n",
      "Epoch: [28][200/391]\tTime 0.037 (0.043)\tData 0.002 (0.002)\tLoss 0.3986 (0.3068)\tPrec 85.156% (89.494%)\n",
      "Epoch: [28][300/391]\tTime 0.036 (0.044)\tData 0.002 (0.002)\tLoss 0.2858 (0.3063)\tPrec 89.844% (89.480%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.108 (0.108)\tLoss 0.3567 (0.3567)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.210% \n",
      "best acc: 85.510000\n",
      "Epoch: [29][0/391]\tTime 0.175 (0.175)\tData 0.129 (0.129)\tLoss 0.2553 (0.2553)\tPrec 91.406% (91.406%)\n",
      "Epoch: [29][100/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 0.2900 (0.2893)\tPrec 89.062% (89.805%)\n",
      "Epoch: [29][200/391]\tTime 0.042 (0.045)\tData 0.002 (0.002)\tLoss 0.4007 (0.2985)\tPrec 85.938% (89.548%)\n",
      "Epoch: [29][300/391]\tTime 0.044 (0.044)\tData 0.001 (0.002)\tLoss 0.3052 (0.2994)\tPrec 89.844% (89.574%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.118 (0.118)\tLoss 0.3654 (0.3654)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.690% \n",
      "best acc: 85.510000\n",
      "Epoch: [30][0/391]\tTime 0.178 (0.178)\tData 0.123 (0.123)\tLoss 0.2944 (0.2944)\tPrec 89.062% (89.062%)\n",
      "Epoch: [30][100/391]\tTime 0.041 (0.046)\tData 0.002 (0.003)\tLoss 0.4300 (0.2916)\tPrec 85.938% (90.060%)\n",
      "Epoch: [30][200/391]\tTime 0.042 (0.047)\tData 0.002 (0.002)\tLoss 0.3164 (0.2910)\tPrec 86.719% (90.077%)\n",
      "Epoch: [30][300/391]\tTime 0.044 (0.046)\tData 0.002 (0.002)\tLoss 0.3525 (0.2959)\tPrec 86.719% (89.815%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 0.4228 (0.4228)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.810% \n",
      "best acc: 85.510000\n",
      "Epoch: [31][0/391]\tTime 0.167 (0.167)\tData 0.122 (0.122)\tLoss 0.3332 (0.3332)\tPrec 89.062% (89.062%)\n",
      "Epoch: [31][100/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.2863 (0.2702)\tPrec 89.062% (90.424%)\n",
      "Epoch: [31][200/391]\tTime 0.050 (0.048)\tData 0.002 (0.003)\tLoss 0.4009 (0.2823)\tPrec 87.500% (89.988%)\n",
      "Epoch: [31][300/391]\tTime 0.043 (0.048)\tData 0.002 (0.002)\tLoss 0.2951 (0.2866)\tPrec 89.062% (89.862%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 0.6272 (0.6272)\tPrec 79.688% (79.688%)\n",
      " * Prec 78.630% \n",
      "best acc: 85.510000\n",
      "Epoch: [32][0/391]\tTime 0.164 (0.164)\tData 0.120 (0.120)\tLoss 0.1685 (0.1685)\tPrec 95.312% (95.312%)\n",
      "Epoch: [32][100/391]\tTime 0.047 (0.045)\tData 0.002 (0.003)\tLoss 0.2300 (0.2768)\tPrec 91.406% (90.401%)\n",
      "Epoch: [32][200/391]\tTime 0.044 (0.044)\tData 0.002 (0.002)\tLoss 0.3244 (0.2812)\tPrec 88.281% (90.116%)\n",
      "Epoch: [32][300/391]\tTime 0.044 (0.044)\tData 0.001 (0.002)\tLoss 0.3401 (0.2837)\tPrec 89.062% (90.036%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.119 (0.119)\tLoss 0.3940 (0.3940)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.040% \n",
      "best acc: 86.040000\n",
      "Epoch: [33][0/391]\tTime 0.166 (0.166)\tData 0.121 (0.121)\tLoss 0.1720 (0.1720)\tPrec 93.750% (93.750%)\n",
      "Epoch: [33][100/391]\tTime 0.055 (0.047)\tData 0.002 (0.003)\tLoss 0.2744 (0.2709)\tPrec 92.188% (90.818%)\n",
      "Epoch: [33][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.3005 (0.2731)\tPrec 88.281% (90.629%)\n",
      "Epoch: [33][300/391]\tTime 0.049 (0.045)\tData 0.002 (0.002)\tLoss 0.2363 (0.2804)\tPrec 93.750% (90.355%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.5816 (0.5816)\tPrec 85.938% (85.938%)\n",
      " * Prec 82.550% \n",
      "best acc: 86.040000\n",
      "Epoch: [34][0/391]\tTime 0.189 (0.189)\tData 0.138 (0.138)\tLoss 0.2356 (0.2356)\tPrec 91.406% (91.406%)\n",
      "Epoch: [34][100/391]\tTime 0.041 (0.044)\tData 0.002 (0.003)\tLoss 0.3305 (0.2745)\tPrec 85.938% (90.486%)\n",
      "Epoch: [34][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.002)\tLoss 0.3078 (0.2748)\tPrec 87.500% (90.365%)\n",
      "Epoch: [34][300/391]\tTime 0.041 (0.043)\tData 0.001 (0.002)\tLoss 0.2112 (0.2784)\tPrec 93.750% (90.254%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.126 (0.126)\tLoss 0.4467 (0.4467)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.390% \n",
      "best acc: 86.040000\n",
      "Epoch: [35][0/391]\tTime 0.174 (0.174)\tData 0.131 (0.131)\tLoss 0.2378 (0.2378)\tPrec 93.750% (93.750%)\n",
      "Epoch: [35][100/391]\tTime 0.043 (0.044)\tData 0.002 (0.003)\tLoss 0.3421 (0.2644)\tPrec 90.625% (90.780%)\n",
      "Epoch: [35][200/391]\tTime 0.043 (0.043)\tData 0.002 (0.002)\tLoss 0.2661 (0.2685)\tPrec 90.625% (90.574%)\n",
      "Epoch: [35][300/391]\tTime 0.045 (0.043)\tData 0.002 (0.002)\tLoss 0.2897 (0.2725)\tPrec 89.844% (90.459%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.120 (0.120)\tLoss 0.3194 (0.3194)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.350% \n",
      "best acc: 86.040000\n",
      "Epoch: [36][0/391]\tTime 0.180 (0.180)\tData 0.120 (0.120)\tLoss 0.1899 (0.1899)\tPrec 95.312% (95.312%)\n",
      "Epoch: [36][100/391]\tTime 0.043 (0.043)\tData 0.002 (0.003)\tLoss 0.1690 (0.2655)\tPrec 93.750% (90.463%)\n",
      "Epoch: [36][200/391]\tTime 0.042 (0.044)\tData 0.002 (0.002)\tLoss 0.1817 (0.2656)\tPrec 92.969% (90.582%)\n",
      "Epoch: [36][300/391]\tTime 0.050 (0.044)\tData 0.001 (0.002)\tLoss 0.3335 (0.2693)\tPrec 90.625% (90.516%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.121 (0.121)\tLoss 0.2973 (0.2973)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.000% \n",
      "best acc: 86.040000\n",
      "Epoch: [37][0/391]\tTime 0.191 (0.191)\tData 0.139 (0.139)\tLoss 0.2488 (0.2488)\tPrec 89.062% (89.062%)\n",
      "Epoch: [37][100/391]\tTime 0.046 (0.045)\tData 0.002 (0.003)\tLoss 0.4056 (0.2546)\tPrec 82.812% (90.834%)\n",
      "Epoch: [37][200/391]\tTime 0.042 (0.045)\tData 0.002 (0.003)\tLoss 0.2568 (0.2600)\tPrec 91.406% (90.730%)\n",
      "Epoch: [37][300/391]\tTime 0.049 (0.047)\tData 0.005 (0.002)\tLoss 0.2054 (0.2664)\tPrec 92.969% (90.508%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.3309 (0.3309)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.470% \n",
      "best acc: 86.040000\n",
      "Epoch: [38][0/391]\tTime 0.163 (0.163)\tData 0.118 (0.118)\tLoss 0.2568 (0.2568)\tPrec 92.188% (92.188%)\n",
      "Epoch: [38][100/391]\tTime 0.051 (0.050)\tData 0.008 (0.003)\tLoss 0.2199 (0.2494)\tPrec 93.750% (91.035%)\n",
      "Epoch: [38][200/391]\tTime 0.060 (0.051)\tData 0.003 (0.003)\tLoss 0.1973 (0.2609)\tPrec 92.969% (90.769%)\n",
      "Epoch: [38][300/391]\tTime 0.060 (0.053)\tData 0.002 (0.003)\tLoss 0.2371 (0.2617)\tPrec 91.406% (90.791%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 0.3661 (0.3661)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.350% \n",
      "best acc: 86.040000\n",
      "Epoch: [39][0/391]\tTime 0.176 (0.176)\tData 0.121 (0.121)\tLoss 0.2101 (0.2101)\tPrec 92.969% (92.969%)\n",
      "Epoch: [39][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.2670 (0.2541)\tPrec 89.062% (91.004%)\n",
      "Epoch: [39][200/391]\tTime 0.057 (0.050)\tData 0.002 (0.003)\tLoss 0.3112 (0.2522)\tPrec 89.844% (91.134%)\n",
      "Epoch: [39][300/391]\tTime 0.055 (0.050)\tData 0.002 (0.002)\tLoss 0.3068 (0.2566)\tPrec 89.844% (90.957%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.4626 (0.4626)\tPrec 82.031% (82.031%)\n",
      " * Prec 82.240% \n",
      "best acc: 86.040000\n",
      "Epoch: [40][0/391]\tTime 0.188 (0.188)\tData 0.132 (0.132)\tLoss 0.1881 (0.1881)\tPrec 92.969% (92.969%)\n",
      "Epoch: [40][100/391]\tTime 0.048 (0.055)\tData 0.002 (0.004)\tLoss 0.1700 (0.2420)\tPrec 92.969% (91.484%)\n",
      "Epoch: [40][200/391]\tTime 0.075 (0.055)\tData 0.003 (0.003)\tLoss 0.2255 (0.2482)\tPrec 93.750% (91.286%)\n",
      "Epoch: [40][300/391]\tTime 0.056 (0.054)\tData 0.003 (0.003)\tLoss 0.2441 (0.2554)\tPrec 92.188% (91.048%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.144 (0.144)\tLoss 0.3395 (0.3395)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.770% \n",
      "best acc: 86.040000\n",
      "Epoch: [41][0/391]\tTime 0.168 (0.168)\tData 0.121 (0.121)\tLoss 0.2158 (0.2158)\tPrec 92.188% (92.188%)\n",
      "Epoch: [41][100/391]\tTime 0.060 (0.052)\tData 0.002 (0.003)\tLoss 0.2414 (0.2519)\tPrec 92.188% (91.259%)\n",
      "Epoch: [41][200/391]\tTime 0.053 (0.054)\tData 0.002 (0.003)\tLoss 0.2109 (0.2486)\tPrec 90.625% (91.290%)\n",
      "Epoch: [41][300/391]\tTime 0.062 (0.054)\tData 0.003 (0.003)\tLoss 0.3115 (0.2526)\tPrec 89.844% (91.160%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.2676 (0.2676)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.000% \n",
      "best acc: 86.040000\n",
      "Epoch: [42][0/391]\tTime 0.198 (0.198)\tData 0.137 (0.137)\tLoss 0.2546 (0.2546)\tPrec 90.625% (90.625%)\n",
      "Epoch: [42][100/391]\tTime 0.048 (0.053)\tData 0.002 (0.003)\tLoss 0.2216 (0.2422)\tPrec 94.531% (91.669%)\n",
      "Epoch: [42][200/391]\tTime 0.058 (0.052)\tData 0.002 (0.003)\tLoss 0.2588 (0.2433)\tPrec 89.062% (91.507%)\n",
      "Epoch: [42][300/391]\tTime 0.062 (0.053)\tData 0.003 (0.003)\tLoss 0.1736 (0.2468)\tPrec 94.531% (91.323%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 0.3897 (0.3897)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.160% \n",
      "best acc: 86.040000\n",
      "Epoch: [43][0/391]\tTime 0.179 (0.179)\tData 0.122 (0.122)\tLoss 0.1732 (0.1732)\tPrec 93.750% (93.750%)\n",
      "Epoch: [43][100/391]\tTime 0.061 (0.054)\tData 0.003 (0.003)\tLoss 0.2956 (0.2439)\tPrec 93.750% (91.151%)\n",
      "Epoch: [43][200/391]\tTime 0.039 (0.053)\tData 0.002 (0.003)\tLoss 0.2913 (0.2492)\tPrec 87.500% (91.134%)\n",
      "Epoch: [43][300/391]\tTime 0.049 (0.053)\tData 0.002 (0.003)\tLoss 0.3508 (0.2479)\tPrec 86.719% (91.193%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 0.3612 (0.3612)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.850% \n",
      "best acc: 86.040000\n",
      "Epoch: [44][0/391]\tTime 0.192 (0.192)\tData 0.135 (0.135)\tLoss 0.1805 (0.1805)\tPrec 93.750% (93.750%)\n",
      "Epoch: [44][100/391]\tTime 0.052 (0.049)\tData 0.002 (0.003)\tLoss 0.2554 (0.2286)\tPrec 91.406% (92.025%)\n",
      "Epoch: [44][200/391]\tTime 0.048 (0.049)\tData 0.002 (0.003)\tLoss 0.2577 (0.2378)\tPrec 89.844% (91.558%)\n",
      "Epoch: [44][300/391]\tTime 0.057 (0.050)\tData 0.002 (0.002)\tLoss 0.2811 (0.2433)\tPrec 90.625% (91.378%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.121 (0.121)\tLoss 0.3554 (0.3554)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.870% \n",
      "best acc: 86.040000\n",
      "Epoch: [45][0/391]\tTime 0.183 (0.183)\tData 0.127 (0.127)\tLoss 0.1875 (0.1875)\tPrec 94.531% (94.531%)\n",
      "Epoch: [45][100/391]\tTime 0.059 (0.056)\tData 0.002 (0.004)\tLoss 0.2010 (0.2291)\tPrec 92.188% (92.211%)\n",
      "Epoch: [45][200/391]\tTime 0.054 (0.054)\tData 0.002 (0.003)\tLoss 0.2922 (0.2318)\tPrec 88.281% (91.908%)\n",
      "Epoch: [45][300/391]\tTime 0.045 (0.054)\tData 0.002 (0.003)\tLoss 0.2420 (0.2363)\tPrec 91.406% (91.694%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.2794 (0.2794)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.380% \n",
      "best acc: 86.380000\n",
      "Epoch: [46][0/391]\tTime 0.172 (0.172)\tData 0.125 (0.125)\tLoss 0.2517 (0.2517)\tPrec 92.969% (92.969%)\n",
      "Epoch: [46][100/391]\tTime 0.061 (0.059)\tData 0.002 (0.004)\tLoss 0.1415 (0.2405)\tPrec 94.531% (91.360%)\n",
      "Epoch: [46][200/391]\tTime 0.054 (0.057)\tData 0.003 (0.003)\tLoss 0.2171 (0.2372)\tPrec 91.406% (91.604%)\n",
      "Epoch: [46][300/391]\tTime 0.054 (0.056)\tData 0.002 (0.003)\tLoss 0.1827 (0.2430)\tPrec 92.969% (91.380%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.3064 (0.3064)\tPrec 92.188% (92.188%)\n",
      " * Prec 86.860% \n",
      "best acc: 86.860000\n",
      "Epoch: [47][0/391]\tTime 0.189 (0.189)\tData 0.125 (0.125)\tLoss 0.1403 (0.1403)\tPrec 92.969% (92.969%)\n",
      "Epoch: [47][100/391]\tTime 0.065 (0.058)\tData 0.003 (0.004)\tLoss 0.2089 (0.2229)\tPrec 92.188% (92.358%)\n",
      "Epoch: [47][200/391]\tTime 0.047 (0.055)\tData 0.002 (0.003)\tLoss 0.1765 (0.2332)\tPrec 95.312% (91.888%)\n",
      "Epoch: [47][300/391]\tTime 0.061 (0.054)\tData 0.002 (0.003)\tLoss 0.2264 (0.2338)\tPrec 92.969% (91.816%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.123 (0.123)\tLoss 0.3174 (0.3174)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.760% \n",
      "best acc: 86.860000\n",
      "Epoch: [48][0/391]\tTime 0.177 (0.177)\tData 0.119 (0.119)\tLoss 0.2199 (0.2199)\tPrec 91.406% (91.406%)\n",
      "Epoch: [48][100/391]\tTime 0.062 (0.055)\tData 0.003 (0.004)\tLoss 0.3286 (0.2223)\tPrec 87.500% (92.110%)\n",
      "Epoch: [48][200/391]\tTime 0.058 (0.056)\tData 0.002 (0.003)\tLoss 0.2579 (0.2285)\tPrec 89.062% (91.966%)\n",
      "Epoch: [48][300/391]\tTime 0.050 (0.057)\tData 0.002 (0.003)\tLoss 0.2234 (0.2385)\tPrec 90.625% (91.562%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.3138 (0.3138)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.280% \n",
      "best acc: 86.860000\n",
      "Epoch: [49][0/391]\tTime 0.181 (0.181)\tData 0.135 (0.135)\tLoss 0.2911 (0.2911)\tPrec 88.281% (88.281%)\n",
      "Epoch: [49][100/391]\tTime 0.046 (0.052)\tData 0.002 (0.003)\tLoss 0.2052 (0.2258)\tPrec 93.750% (92.071%)\n",
      "Epoch: [49][200/391]\tTime 0.040 (0.051)\tData 0.002 (0.003)\tLoss 0.2067 (0.2379)\tPrec 94.531% (91.702%)\n",
      "Epoch: [49][300/391]\tTime 0.047 (0.049)\tData 0.003 (0.003)\tLoss 0.1650 (0.2403)\tPrec 92.969% (91.629%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.2402 (0.2402)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.340% \n",
      "best acc: 87.340000\n",
      "Epoch: [50][0/391]\tTime 0.184 (0.184)\tData 0.130 (0.130)\tLoss 0.2498 (0.2498)\tPrec 92.188% (92.188%)\n",
      "Epoch: [50][100/391]\tTime 0.053 (0.055)\tData 0.002 (0.003)\tLoss 0.2491 (0.2197)\tPrec 92.188% (92.288%)\n",
      "Epoch: [50][200/391]\tTime 0.043 (0.054)\tData 0.002 (0.003)\tLoss 0.3341 (0.2242)\tPrec 85.938% (92.013%)\n",
      "Epoch: [50][300/391]\tTime 0.053 (0.055)\tData 0.002 (0.003)\tLoss 0.2550 (0.2292)\tPrec 90.625% (91.956%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.3213 (0.3213)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.450% \n",
      "best acc: 87.340000\n",
      "Epoch: [51][0/391]\tTime 0.183 (0.183)\tData 0.125 (0.125)\tLoss 0.1975 (0.1975)\tPrec 93.750% (93.750%)\n",
      "Epoch: [51][100/391]\tTime 0.052 (0.052)\tData 0.002 (0.003)\tLoss 0.1842 (0.2144)\tPrec 93.750% (92.265%)\n",
      "Epoch: [51][200/391]\tTime 0.049 (0.054)\tData 0.002 (0.003)\tLoss 0.2586 (0.2161)\tPrec 92.188% (92.265%)\n",
      "Epoch: [51][300/391]\tTime 0.049 (0.054)\tData 0.002 (0.003)\tLoss 0.2861 (0.2186)\tPrec 90.625% (92.237%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.121 (0.121)\tLoss 0.3005 (0.3005)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.490% \n",
      "best acc: 87.340000\n",
      "Epoch: [52][0/391]\tTime 0.190 (0.190)\tData 0.133 (0.133)\tLoss 0.1397 (0.1397)\tPrec 92.969% (92.969%)\n",
      "Epoch: [52][100/391]\tTime 0.058 (0.054)\tData 0.002 (0.004)\tLoss 0.3405 (0.2231)\tPrec 88.281% (92.071%)\n",
      "Epoch: [52][200/391]\tTime 0.061 (0.054)\tData 0.002 (0.003)\tLoss 0.2332 (0.2287)\tPrec 93.750% (91.904%)\n",
      "Epoch: [52][300/391]\tTime 0.053 (0.054)\tData 0.002 (0.003)\tLoss 0.2650 (0.2281)\tPrec 89.844% (91.998%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.119 (0.119)\tLoss 0.2611 (0.2611)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.530% \n",
      "best acc: 87.340000\n",
      "Epoch: [53][0/391]\tTime 0.177 (0.177)\tData 0.126 (0.126)\tLoss 0.1820 (0.1820)\tPrec 93.750% (93.750%)\n",
      "Epoch: [53][100/391]\tTime 0.057 (0.050)\tData 0.002 (0.003)\tLoss 0.2444 (0.2220)\tPrec 92.188% (92.180%)\n",
      "Epoch: [53][200/391]\tTime 0.058 (0.054)\tData 0.003 (0.003)\tLoss 0.2178 (0.2264)\tPrec 92.188% (92.009%)\n",
      "Epoch: [53][300/391]\tTime 0.062 (0.056)\tData 0.002 (0.003)\tLoss 0.1655 (0.2262)\tPrec 92.969% (92.037%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.126 (0.126)\tLoss 0.4206 (0.4206)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.880% \n",
      "best acc: 87.340000\n",
      "Epoch: [54][0/391]\tTime 0.166 (0.166)\tData 0.119 (0.119)\tLoss 0.2126 (0.2126)\tPrec 93.750% (93.750%)\n",
      "Epoch: [54][100/391]\tTime 0.050 (0.057)\tData 0.002 (0.004)\tLoss 0.2409 (0.2097)\tPrec 91.406% (92.667%)\n",
      "Epoch: [54][200/391]\tTime 0.057 (0.058)\tData 0.002 (0.003)\tLoss 0.1602 (0.2182)\tPrec 93.750% (92.308%)\n",
      "Epoch: [54][300/391]\tTime 0.060 (0.057)\tData 0.002 (0.003)\tLoss 0.2245 (0.2263)\tPrec 91.406% (92.040%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.3321 (0.3321)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.240% \n",
      "best acc: 87.340000\n",
      "Epoch: [55][0/391]\tTime 0.163 (0.163)\tData 0.118 (0.118)\tLoss 0.3414 (0.3414)\tPrec 89.844% (89.844%)\n",
      "Epoch: [55][100/391]\tTime 0.045 (0.055)\tData 0.002 (0.003)\tLoss 0.2399 (0.2120)\tPrec 92.188% (92.474%)\n",
      "Epoch: [55][200/391]\tTime 0.054 (0.054)\tData 0.002 (0.003)\tLoss 0.2473 (0.2176)\tPrec 89.844% (92.238%)\n",
      "Epoch: [55][300/391]\tTime 0.055 (0.053)\tData 0.003 (0.003)\tLoss 0.2042 (0.2225)\tPrec 92.188% (92.032%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 0.3504 (0.3504)\tPrec 90.625% (90.625%)\n",
      " * Prec 84.900% \n",
      "best acc: 87.340000\n",
      "Epoch: [56][0/391]\tTime 0.184 (0.184)\tData 0.123 (0.123)\tLoss 0.2604 (0.2604)\tPrec 92.188% (92.188%)\n",
      "Epoch: [56][100/391]\tTime 0.053 (0.055)\tData 0.002 (0.003)\tLoss 0.1620 (0.2121)\tPrec 94.531% (92.543%)\n",
      "Epoch: [56][200/391]\tTime 0.044 (0.053)\tData 0.002 (0.003)\tLoss 0.2126 (0.2178)\tPrec 92.969% (92.359%)\n",
      "Epoch: [56][300/391]\tTime 0.054 (0.052)\tData 0.002 (0.002)\tLoss 0.2997 (0.2230)\tPrec 87.500% (92.206%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.2289 (0.2289)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.070% \n",
      "best acc: 87.340000\n",
      "Epoch: [57][0/391]\tTime 0.171 (0.171)\tData 0.124 (0.124)\tLoss 0.1421 (0.1421)\tPrec 95.312% (95.312%)\n",
      "Epoch: [57][100/391]\tTime 0.051 (0.048)\tData 0.002 (0.003)\tLoss 0.2635 (0.2050)\tPrec 88.281% (92.613%)\n",
      "Epoch: [57][200/391]\tTime 0.045 (0.050)\tData 0.002 (0.003)\tLoss 0.1589 (0.2071)\tPrec 93.750% (92.533%)\n",
      "Epoch: [57][300/391]\tTime 0.057 (0.050)\tData 0.003 (0.003)\tLoss 0.1572 (0.2164)\tPrec 93.750% (92.180%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.3277 (0.3277)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.060% \n",
      "best acc: 88.060000\n",
      "Epoch: [58][0/391]\tTime 0.176 (0.176)\tData 0.132 (0.132)\tLoss 0.1899 (0.1899)\tPrec 92.969% (92.969%)\n",
      "Epoch: [58][100/391]\tTime 0.049 (0.054)\tData 0.002 (0.004)\tLoss 0.2725 (0.1986)\tPrec 89.844% (92.891%)\n",
      "Epoch: [58][200/391]\tTime 0.048 (0.051)\tData 0.002 (0.003)\tLoss 0.2995 (0.2058)\tPrec 92.188% (92.681%)\n",
      "Epoch: [58][300/391]\tTime 0.049 (0.052)\tData 0.002 (0.003)\tLoss 0.2780 (0.2161)\tPrec 93.750% (92.271%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.2396 (0.2396)\tPrec 92.969% (92.969%)\n",
      " * Prec 87.820% \n",
      "best acc: 88.060000\n",
      "Epoch: [59][0/391]\tTime 0.184 (0.184)\tData 0.119 (0.119)\tLoss 0.1338 (0.1338)\tPrec 96.094% (96.094%)\n",
      "Epoch: [59][100/391]\tTime 0.048 (0.052)\tData 0.002 (0.003)\tLoss 0.2267 (0.2047)\tPrec 93.750% (92.799%)\n",
      "Epoch: [59][200/391]\tTime 0.049 (0.053)\tData 0.001 (0.003)\tLoss 0.2266 (0.2135)\tPrec 92.188% (92.448%)\n",
      "Epoch: [59][300/391]\tTime 0.045 (0.052)\tData 0.002 (0.002)\tLoss 0.2545 (0.2131)\tPrec 92.969% (92.478%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.3245 (0.3245)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.610% \n",
      "best acc: 88.060000\n",
      "Epoch: [60][0/391]\tTime 0.182 (0.182)\tData 0.126 (0.126)\tLoss 0.1634 (0.1634)\tPrec 96.094% (96.094%)\n",
      "Epoch: [60][100/391]\tTime 0.057 (0.057)\tData 0.002 (0.003)\tLoss 0.2904 (0.2153)\tPrec 88.281% (92.474%)\n",
      "Epoch: [60][200/391]\tTime 0.057 (0.058)\tData 0.003 (0.003)\tLoss 0.1427 (0.2078)\tPrec 93.750% (92.631%)\n",
      "Epoch: [60][300/391]\tTime 0.045 (0.057)\tData 0.002 (0.003)\tLoss 0.1568 (0.2076)\tPrec 95.312% (92.670%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.124 (0.124)\tLoss 0.4026 (0.4026)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.060% \n",
      "best acc: 88.060000\n",
      "Epoch: [61][0/391]\tTime 0.208 (0.208)\tData 0.158 (0.158)\tLoss 0.1955 (0.1955)\tPrec 91.406% (91.406%)\n",
      "Epoch: [61][100/391]\tTime 0.054 (0.058)\tData 0.002 (0.004)\tLoss 0.1819 (0.2087)\tPrec 92.969% (92.427%)\n",
      "Epoch: [61][200/391]\tTime 0.045 (0.054)\tData 0.002 (0.003)\tLoss 0.2331 (0.2173)\tPrec 91.406% (92.118%)\n",
      "Epoch: [61][300/391]\tTime 0.056 (0.055)\tData 0.002 (0.003)\tLoss 0.2163 (0.2163)\tPrec 93.750% (92.151%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.2051 (0.2051)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.270% \n",
      "best acc: 88.060000\n",
      "Epoch: [62][0/391]\tTime 0.184 (0.184)\tData 0.122 (0.122)\tLoss 0.1285 (0.1285)\tPrec 95.312% (95.312%)\n",
      "Epoch: [62][100/391]\tTime 0.062 (0.061)\tData 0.003 (0.004)\tLoss 0.2078 (0.1994)\tPrec 92.969% (92.969%)\n",
      "Epoch: [62][200/391]\tTime 0.052 (0.058)\tData 0.002 (0.003)\tLoss 0.3141 (0.2049)\tPrec 90.625% (92.852%)\n",
      "Epoch: [62][300/391]\tTime 0.050 (0.056)\tData 0.002 (0.003)\tLoss 0.1704 (0.2091)\tPrec 93.750% (92.701%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.132 (0.132)\tLoss 0.3450 (0.3450)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.090% \n",
      "best acc: 88.060000\n",
      "Epoch: [63][0/391]\tTime 0.174 (0.174)\tData 0.125 (0.125)\tLoss 0.1944 (0.1944)\tPrec 92.969% (92.969%)\n",
      "Epoch: [63][100/391]\tTime 0.055 (0.054)\tData 0.002 (0.003)\tLoss 0.1730 (0.1905)\tPrec 95.312% (93.263%)\n",
      "Epoch: [63][200/391]\tTime 0.056 (0.056)\tData 0.002 (0.003)\tLoss 0.1889 (0.1988)\tPrec 94.531% (93.008%)\n",
      "Epoch: [63][300/391]\tTime 0.066 (0.055)\tData 0.005 (0.003)\tLoss 0.1819 (0.2068)\tPrec 94.531% (92.746%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.153 (0.153)\tLoss 0.3971 (0.3971)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.430% \n",
      "best acc: 88.060000\n",
      "Epoch: [64][0/391]\tTime 0.195 (0.195)\tData 0.125 (0.125)\tLoss 0.1399 (0.1399)\tPrec 94.531% (94.531%)\n",
      "Epoch: [64][100/391]\tTime 0.056 (0.058)\tData 0.002 (0.004)\tLoss 0.1668 (0.2066)\tPrec 93.750% (92.938%)\n",
      "Epoch: [64][200/391]\tTime 0.047 (0.057)\tData 0.002 (0.003)\tLoss 0.1834 (0.2028)\tPrec 91.406% (92.984%)\n",
      "Epoch: [64][300/391]\tTime 0.046 (0.056)\tData 0.002 (0.003)\tLoss 0.1329 (0.2047)\tPrec 95.312% (92.831%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.3835 (0.3835)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.340% \n",
      "best acc: 88.060000\n",
      "Epoch: [65][0/391]\tTime 0.203 (0.203)\tData 0.154 (0.154)\tLoss 0.1464 (0.1464)\tPrec 93.750% (93.750%)\n",
      "Epoch: [65][100/391]\tTime 0.057 (0.056)\tData 0.003 (0.004)\tLoss 0.2144 (0.2016)\tPrec 92.969% (92.636%)\n",
      "Epoch: [65][200/391]\tTime 0.055 (0.055)\tData 0.002 (0.003)\tLoss 0.3408 (0.2071)\tPrec 88.281% (92.642%)\n",
      "Epoch: [65][300/391]\tTime 0.047 (0.053)\tData 0.002 (0.003)\tLoss 0.2135 (0.2097)\tPrec 93.750% (92.616%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.131 (0.131)\tLoss 0.3786 (0.3786)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.990% \n",
      "best acc: 88.060000\n",
      "Epoch: [66][0/391]\tTime 0.180 (0.180)\tData 0.126 (0.126)\tLoss 0.1258 (0.1258)\tPrec 94.531% (94.531%)\n",
      "Epoch: [66][100/391]\tTime 0.057 (0.051)\tData 0.003 (0.004)\tLoss 0.0934 (0.1891)\tPrec 96.094% (93.232%)\n",
      "Epoch: [66][200/391]\tTime 0.060 (0.051)\tData 0.002 (0.003)\tLoss 0.2062 (0.2028)\tPrec 90.625% (92.798%)\n",
      "Epoch: [66][300/391]\tTime 0.049 (0.051)\tData 0.002 (0.003)\tLoss 0.1644 (0.2048)\tPrec 95.312% (92.784%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.3023 (0.3023)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.880% \n",
      "best acc: 88.060000\n",
      "Epoch: [67][0/391]\tTime 0.196 (0.196)\tData 0.130 (0.130)\tLoss 0.1588 (0.1588)\tPrec 94.531% (94.531%)\n",
      "Epoch: [67][100/391]\tTime 0.056 (0.060)\tData 0.002 (0.004)\tLoss 0.2215 (0.2013)\tPrec 89.844% (92.752%)\n",
      "Epoch: [67][200/391]\tTime 0.062 (0.058)\tData 0.002 (0.003)\tLoss 0.1609 (0.2014)\tPrec 95.312% (92.720%)\n",
      "Epoch: [67][300/391]\tTime 0.063 (0.057)\tData 0.003 (0.003)\tLoss 0.1813 (0.2020)\tPrec 94.531% (92.769%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.1722 (0.1722)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.700% \n",
      "best acc: 88.060000\n",
      "Epoch: [68][0/391]\tTime 0.171 (0.171)\tData 0.122 (0.122)\tLoss 0.2164 (0.2164)\tPrec 91.406% (91.406%)\n",
      "Epoch: [68][100/391]\tTime 0.054 (0.047)\tData 0.002 (0.003)\tLoss 0.1302 (0.1922)\tPrec 94.531% (93.147%)\n",
      "Epoch: [68][200/391]\tTime 0.043 (0.049)\tData 0.002 (0.003)\tLoss 0.2375 (0.1980)\tPrec 92.969% (92.988%)\n",
      "Epoch: [68][300/391]\tTime 0.062 (0.050)\tData 0.003 (0.003)\tLoss 0.1303 (0.2026)\tPrec 95.312% (92.823%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.144 (0.144)\tLoss 0.3254 (0.3254)\tPrec 90.625% (90.625%)\n",
      " * Prec 84.450% \n",
      "best acc: 88.060000\n",
      "Epoch: [69][0/391]\tTime 0.193 (0.193)\tData 0.135 (0.135)\tLoss 0.2018 (0.2018)\tPrec 94.531% (94.531%)\n",
      "Epoch: [69][100/391]\tTime 0.051 (0.056)\tData 0.002 (0.004)\tLoss 0.1370 (0.1890)\tPrec 93.750% (93.417%)\n",
      "Epoch: [69][200/391]\tTime 0.061 (0.058)\tData 0.002 (0.003)\tLoss 0.1718 (0.2012)\tPrec 93.750% (92.953%)\n",
      "Epoch: [69][300/391]\tTime 0.048 (0.058)\tData 0.002 (0.003)\tLoss 0.1144 (0.2036)\tPrec 95.312% (92.777%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.127 (0.127)\tLoss 0.3569 (0.3569)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.260% \n",
      "best acc: 88.060000\n",
      "Epoch: [70][0/391]\tTime 0.201 (0.201)\tData 0.130 (0.130)\tLoss 0.2649 (0.2649)\tPrec 89.062% (89.062%)\n",
      "Epoch: [70][100/391]\tTime 0.067 (0.061)\tData 0.002 (0.004)\tLoss 0.1236 (0.1875)\tPrec 96.094% (93.502%)\n",
      "Epoch: [70][200/391]\tTime 0.063 (0.064)\tData 0.002 (0.003)\tLoss 0.1784 (0.1919)\tPrec 92.969% (93.280%)\n",
      "Epoch: [70][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.003)\tLoss 0.2552 (0.1962)\tPrec 89.062% (93.109%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.2333 (0.2333)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.430% \n",
      "best acc: 88.060000\n",
      "Epoch: [71][0/391]\tTime 0.201 (0.201)\tData 0.131 (0.131)\tLoss 0.1027 (0.1027)\tPrec 98.438% (98.438%)\n",
      "Epoch: [71][100/391]\tTime 0.060 (0.064)\tData 0.002 (0.004)\tLoss 0.2121 (0.1837)\tPrec 90.625% (93.619%)\n",
      "Epoch: [71][200/391]\tTime 0.062 (0.061)\tData 0.002 (0.003)\tLoss 0.2829 (0.1881)\tPrec 89.062% (93.435%)\n",
      "Epoch: [71][300/391]\tTime 0.066 (0.061)\tData 0.002 (0.003)\tLoss 0.1686 (0.1973)\tPrec 92.969% (93.023%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.3339 (0.3339)\tPrec 86.719% (86.719%)\n",
      " * Prec 87.380% \n",
      "best acc: 88.060000\n",
      "Epoch: [72][0/391]\tTime 0.187 (0.187)\tData 0.119 (0.119)\tLoss 0.1621 (0.1621)\tPrec 94.531% (94.531%)\n",
      "Epoch: [72][100/391]\tTime 0.060 (0.065)\tData 0.002 (0.003)\tLoss 0.1399 (0.1877)\tPrec 93.750% (93.301%)\n",
      "Epoch: [72][200/391]\tTime 0.067 (0.065)\tData 0.002 (0.003)\tLoss 0.1841 (0.1959)\tPrec 92.188% (92.996%)\n",
      "Epoch: [72][300/391]\tTime 0.068 (0.066)\tData 0.003 (0.003)\tLoss 0.2255 (0.1943)\tPrec 93.750% (93.109%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.137 (0.137)\tLoss 0.4045 (0.4045)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.420% \n",
      "best acc: 88.060000\n",
      "Epoch: [73][0/391]\tTime 0.208 (0.208)\tData 0.140 (0.140)\tLoss 0.2220 (0.2220)\tPrec 93.750% (93.750%)\n",
      "Epoch: [73][100/391]\tTime 0.065 (0.061)\tData 0.002 (0.004)\tLoss 0.2280 (0.1937)\tPrec 93.750% (93.247%)\n",
      "Epoch: [73][200/391]\tTime 0.067 (0.064)\tData 0.002 (0.003)\tLoss 0.2079 (0.1965)\tPrec 91.406% (93.000%)\n",
      "Epoch: [73][300/391]\tTime 0.067 (0.064)\tData 0.002 (0.003)\tLoss 0.1456 (0.1983)\tPrec 95.312% (92.964%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.123 (0.123)\tLoss 0.4655 (0.4655)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.020% \n",
      "best acc: 88.060000\n",
      "Epoch: [74][0/391]\tTime 0.194 (0.194)\tData 0.132 (0.132)\tLoss 0.1588 (0.1588)\tPrec 92.969% (92.969%)\n",
      "Epoch: [74][100/391]\tTime 0.068 (0.064)\tData 0.002 (0.004)\tLoss 0.2023 (0.1895)\tPrec 91.406% (93.325%)\n",
      "Epoch: [74][200/391]\tTime 0.061 (0.065)\tData 0.002 (0.003)\tLoss 0.1923 (0.1904)\tPrec 92.969% (93.322%)\n",
      "Epoch: [74][300/391]\tTime 0.067 (0.065)\tData 0.002 (0.003)\tLoss 0.3431 (0.1942)\tPrec 89.062% (93.140%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.2342 (0.2342)\tPrec 92.969% (92.969%)\n",
      " * Prec 86.320% \n",
      "best acc: 88.060000\n",
      "Epoch: [75][0/391]\tTime 0.204 (0.204)\tData 0.128 (0.128)\tLoss 0.1685 (0.1685)\tPrec 94.531% (94.531%)\n",
      "Epoch: [75][100/391]\tTime 0.064 (0.066)\tData 0.002 (0.004)\tLoss 0.2737 (0.1926)\tPrec 91.406% (93.100%)\n",
      "Epoch: [75][200/391]\tTime 0.064 (0.067)\tData 0.002 (0.003)\tLoss 0.2552 (0.1895)\tPrec 90.625% (93.252%)\n",
      "Epoch: [75][300/391]\tTime 0.070 (0.067)\tData 0.002 (0.003)\tLoss 0.1506 (0.1930)\tPrec 93.750% (93.109%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.2449 (0.2449)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.380% \n",
      "best acc: 88.060000\n",
      "Epoch: [76][0/391]\tTime 0.194 (0.194)\tData 0.125 (0.125)\tLoss 0.1296 (0.1296)\tPrec 95.312% (95.312%)\n",
      "Epoch: [76][100/391]\tTime 0.069 (0.066)\tData 0.003 (0.003)\tLoss 0.1508 (0.1838)\tPrec 96.094% (93.595%)\n",
      "Epoch: [76][200/391]\tTime 0.050 (0.060)\tData 0.002 (0.003)\tLoss 0.1877 (0.1822)\tPrec 92.188% (93.579%)\n",
      "Epoch: [76][300/391]\tTime 0.055 (0.060)\tData 0.002 (0.003)\tLoss 0.2055 (0.1881)\tPrec 92.188% (93.371%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.127 (0.127)\tLoss 0.2480 (0.2480)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.140% \n",
      "best acc: 88.060000\n",
      "Epoch: [77][0/391]\tTime 0.175 (0.175)\tData 0.121 (0.121)\tLoss 0.1191 (0.1191)\tPrec 96.094% (96.094%)\n",
      "Epoch: [77][100/391]\tTime 0.059 (0.050)\tData 0.002 (0.003)\tLoss 0.2065 (0.1762)\tPrec 93.750% (93.665%)\n",
      "Epoch: [77][200/391]\tTime 0.062 (0.054)\tData 0.003 (0.003)\tLoss 0.2715 (0.1925)\tPrec 89.062% (93.175%)\n",
      "Epoch: [77][300/391]\tTime 0.056 (0.055)\tData 0.002 (0.003)\tLoss 0.1505 (0.1959)\tPrec 94.531% (92.992%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.2625 (0.2625)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.910% \n",
      "best acc: 88.060000\n",
      "Epoch: [78][0/391]\tTime 0.174 (0.174)\tData 0.117 (0.117)\tLoss 0.1636 (0.1636)\tPrec 93.750% (93.750%)\n",
      "Epoch: [78][100/391]\tTime 0.062 (0.056)\tData 0.003 (0.003)\tLoss 0.2135 (0.1794)\tPrec 91.406% (93.665%)\n",
      "Epoch: [78][200/391]\tTime 0.064 (0.058)\tData 0.003 (0.003)\tLoss 0.1279 (0.1864)\tPrec 96.094% (93.447%)\n",
      "Epoch: [78][300/391]\tTime 0.049 (0.058)\tData 0.002 (0.003)\tLoss 0.1878 (0.1893)\tPrec 94.531% (93.296%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.132 (0.132)\tLoss 0.3642 (0.3642)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.160% \n",
      "best acc: 88.060000\n",
      "Epoch: [79][0/391]\tTime 0.185 (0.185)\tData 0.130 (0.130)\tLoss 0.1145 (0.1145)\tPrec 96.094% (96.094%)\n",
      "Epoch: [79][100/391]\tTime 0.062 (0.060)\tData 0.002 (0.004)\tLoss 0.1301 (0.1839)\tPrec 95.312% (93.533%)\n",
      "Epoch: [79][200/391]\tTime 0.055 (0.060)\tData 0.002 (0.003)\tLoss 0.1638 (0.1859)\tPrec 93.750% (93.435%)\n",
      "Epoch: [79][300/391]\tTime 0.063 (0.060)\tData 0.002 (0.003)\tLoss 0.3036 (0.1891)\tPrec 88.281% (93.259%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.127 (0.127)\tLoss 0.3749 (0.3749)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.980% \n",
      "best acc: 88.060000\n",
      "Epoch: [80][0/391]\tTime 0.177 (0.177)\tData 0.123 (0.123)\tLoss 0.1952 (0.1952)\tPrec 92.188% (92.188%)\n",
      "Epoch: [80][100/391]\tTime 0.059 (0.062)\tData 0.003 (0.004)\tLoss 0.2270 (0.1815)\tPrec 92.188% (93.688%)\n",
      "Epoch: [80][200/391]\tTime 0.052 (0.062)\tData 0.002 (0.003)\tLoss 0.2758 (0.1903)\tPrec 94.531% (93.385%)\n",
      "Epoch: [80][300/391]\tTime 0.057 (0.061)\tData 0.002 (0.003)\tLoss 0.1908 (0.1910)\tPrec 92.188% (93.319%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.3296 (0.3296)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.890% \n",
      "best acc: 88.060000\n",
      "Epoch: [81][0/391]\tTime 0.181 (0.181)\tData 0.117 (0.117)\tLoss 0.1492 (0.1492)\tPrec 94.531% (94.531%)\n",
      "Epoch: [81][100/391]\tTime 0.050 (0.054)\tData 0.002 (0.003)\tLoss 0.2236 (0.1777)\tPrec 93.750% (93.719%)\n",
      "Epoch: [81][200/391]\tTime 0.060 (0.054)\tData 0.003 (0.003)\tLoss 0.1866 (0.1842)\tPrec 94.531% (93.412%)\n",
      "Epoch: [81][300/391]\tTime 0.059 (0.055)\tData 0.002 (0.003)\tLoss 0.2508 (0.1859)\tPrec 93.750% (93.358%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.125 (0.125)\tLoss 0.2343 (0.2343)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.580% \n",
      "best acc: 88.060000\n",
      "Epoch: [82][0/391]\tTime 0.167 (0.167)\tData 0.115 (0.115)\tLoss 0.2111 (0.2111)\tPrec 91.406% (91.406%)\n",
      "Epoch: [82][100/391]\tTime 0.062 (0.054)\tData 0.002 (0.003)\tLoss 0.1058 (0.1750)\tPrec 95.312% (93.603%)\n",
      "Epoch: [82][200/391]\tTime 0.061 (0.056)\tData 0.003 (0.003)\tLoss 0.3477 (0.1846)\tPrec 89.844% (93.365%)\n",
      "Epoch: [82][300/391]\tTime 0.059 (0.057)\tData 0.003 (0.003)\tLoss 0.1445 (0.1895)\tPrec 95.312% (93.205%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.131 (0.131)\tLoss 0.4439 (0.4439)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.030% \n",
      "best acc: 88.060000\n",
      "Epoch: [83][0/391]\tTime 0.188 (0.188)\tData 0.121 (0.121)\tLoss 0.1476 (0.1476)\tPrec 92.969% (92.969%)\n",
      "Epoch: [83][100/391]\tTime 0.061 (0.057)\tData 0.003 (0.003)\tLoss 0.1090 (0.1715)\tPrec 95.312% (93.866%)\n",
      "Epoch: [83][200/391]\tTime 0.063 (0.057)\tData 0.002 (0.003)\tLoss 0.2450 (0.1836)\tPrec 92.188% (93.563%)\n",
      "Epoch: [83][300/391]\tTime 0.060 (0.058)\tData 0.002 (0.003)\tLoss 0.1484 (0.1891)\tPrec 95.312% (93.340%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.3900 (0.3900)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.600% \n",
      "best acc: 88.060000\n",
      "Epoch: [84][0/391]\tTime 0.188 (0.188)\tData 0.120 (0.120)\tLoss 0.1387 (0.1387)\tPrec 94.531% (94.531%)\n",
      "Epoch: [84][100/391]\tTime 0.067 (0.060)\tData 0.005 (0.004)\tLoss 0.2032 (0.1817)\tPrec 94.531% (93.448%)\n",
      "Epoch: [84][200/391]\tTime 0.059 (0.060)\tData 0.002 (0.003)\tLoss 0.1999 (0.1796)\tPrec 91.406% (93.602%)\n",
      "Epoch: [84][300/391]\tTime 0.062 (0.059)\tData 0.002 (0.003)\tLoss 0.2062 (0.1808)\tPrec 92.969% (93.553%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.3009 (0.3009)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.460% \n",
      "best acc: 88.060000\n",
      "Epoch: [85][0/391]\tTime 0.170 (0.170)\tData 0.116 (0.116)\tLoss 0.1125 (0.1125)\tPrec 96.094% (96.094%)\n",
      "Epoch: [85][100/391]\tTime 0.061 (0.059)\tData 0.002 (0.003)\tLoss 0.2531 (0.1756)\tPrec 89.062% (93.541%)\n",
      "Epoch: [85][200/391]\tTime 0.064 (0.061)\tData 0.002 (0.003)\tLoss 0.1593 (0.1780)\tPrec 92.188% (93.587%)\n",
      "Epoch: [85][300/391]\tTime 0.055 (0.061)\tData 0.002 (0.003)\tLoss 0.1382 (0.1826)\tPrec 96.875% (93.441%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.3987 (0.3987)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.460% \n",
      "best acc: 88.060000\n",
      "Epoch: [86][0/391]\tTime 0.177 (0.177)\tData 0.117 (0.117)\tLoss 0.1905 (0.1905)\tPrec 93.750% (93.750%)\n",
      "Epoch: [86][100/391]\tTime 0.053 (0.060)\tData 0.002 (0.004)\tLoss 0.1896 (0.1758)\tPrec 93.750% (93.634%)\n",
      "Epoch: [86][200/391]\tTime 0.052 (0.058)\tData 0.002 (0.003)\tLoss 0.1589 (0.1804)\tPrec 96.094% (93.587%)\n",
      "Epoch: [86][300/391]\tTime 0.059 (0.057)\tData 0.003 (0.003)\tLoss 0.1787 (0.1825)\tPrec 94.531% (93.529%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.132 (0.132)\tLoss 0.2677 (0.2677)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.990% \n",
      "best acc: 88.060000\n",
      "Epoch: [87][0/391]\tTime 0.179 (0.179)\tData 0.119 (0.119)\tLoss 0.1688 (0.1688)\tPrec 92.969% (92.969%)\n",
      "Epoch: [87][100/391]\tTime 0.054 (0.056)\tData 0.002 (0.003)\tLoss 0.2385 (0.1716)\tPrec 92.969% (94.013%)\n",
      "Epoch: [87][200/391]\tTime 0.061 (0.056)\tData 0.003 (0.003)\tLoss 0.2825 (0.1819)\tPrec 90.625% (93.626%)\n",
      "Epoch: [87][300/391]\tTime 0.052 (0.058)\tData 0.002 (0.003)\tLoss 0.2062 (0.1856)\tPrec 93.750% (93.464%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.3076 (0.3076)\tPrec 88.281% (88.281%)\n",
      " * Prec 88.600% \n",
      "best acc: 88.600000\n",
      "Epoch: [88][0/391]\tTime 0.188 (0.188)\tData 0.126 (0.126)\tLoss 0.1661 (0.1661)\tPrec 93.750% (93.750%)\n",
      "Epoch: [88][100/391]\tTime 0.063 (0.058)\tData 0.002 (0.004)\tLoss 0.3270 (0.1690)\tPrec 87.500% (94.175%)\n",
      "Epoch: [88][200/391]\tTime 0.056 (0.055)\tData 0.003 (0.003)\tLoss 0.1213 (0.1766)\tPrec 96.094% (93.715%)\n",
      "Epoch: [88][300/391]\tTime 0.061 (0.057)\tData 0.003 (0.003)\tLoss 0.1596 (0.1781)\tPrec 94.531% (93.628%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.137 (0.137)\tLoss 0.3348 (0.3348)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.850% \n",
      "best acc: 88.600000\n",
      "Epoch: [89][0/391]\tTime 0.187 (0.187)\tData 0.132 (0.132)\tLoss 0.2160 (0.2160)\tPrec 91.406% (91.406%)\n",
      "Epoch: [89][100/391]\tTime 0.057 (0.054)\tData 0.002 (0.003)\tLoss 0.1688 (0.1767)\tPrec 94.531% (93.696%)\n",
      "Epoch: [89][200/391]\tTime 0.065 (0.054)\tData 0.002 (0.003)\tLoss 0.1911 (0.1786)\tPrec 93.750% (93.595%)\n",
      "Epoch: [89][300/391]\tTime 0.056 (0.053)\tData 0.002 (0.003)\tLoss 0.2445 (0.1830)\tPrec 92.188% (93.472%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.3021 (0.3021)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.500% \n",
      "best acc: 88.600000\n",
      "Epoch: [90][0/391]\tTime 0.182 (0.182)\tData 0.130 (0.130)\tLoss 0.0953 (0.0953)\tPrec 97.656% (97.656%)\n",
      "Epoch: [90][100/391]\tTime 0.058 (0.050)\tData 0.002 (0.003)\tLoss 0.1196 (0.1639)\tPrec 96.094% (94.284%)\n",
      "Epoch: [90][200/391]\tTime 0.063 (0.054)\tData 0.003 (0.003)\tLoss 0.1045 (0.1725)\tPrec 94.531% (93.972%)\n",
      "Epoch: [90][300/391]\tTime 0.054 (0.054)\tData 0.002 (0.003)\tLoss 0.1665 (0.1785)\tPrec 96.094% (93.734%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.3373 (0.3373)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.280% \n",
      "best acc: 88.600000\n",
      "Epoch: [91][0/391]\tTime 0.203 (0.203)\tData 0.138 (0.138)\tLoss 0.1879 (0.1879)\tPrec 92.969% (92.969%)\n",
      "Epoch: [91][100/391]\tTime 0.060 (0.057)\tData 0.002 (0.004)\tLoss 0.1584 (0.1622)\tPrec 94.531% (94.206%)\n",
      "Epoch: [91][200/391]\tTime 0.049 (0.056)\tData 0.002 (0.003)\tLoss 0.2524 (0.1699)\tPrec 91.406% (93.964%)\n",
      "Epoch: [91][300/391]\tTime 0.051 (0.055)\tData 0.002 (0.003)\tLoss 0.1682 (0.1757)\tPrec 93.750% (93.851%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.3737 (0.3737)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.660% \n",
      "best acc: 88.600000\n",
      "Epoch: [92][0/391]\tTime 0.193 (0.193)\tData 0.130 (0.130)\tLoss 0.1151 (0.1151)\tPrec 96.094% (96.094%)\n",
      "Epoch: [92][100/391]\tTime 0.052 (0.055)\tData 0.001 (0.003)\tLoss 0.1674 (0.1613)\tPrec 96.094% (94.237%)\n",
      "Epoch: [92][200/391]\tTime 0.055 (0.055)\tData 0.002 (0.003)\tLoss 0.1294 (0.1714)\tPrec 96.094% (93.902%)\n",
      "Epoch: [92][300/391]\tTime 0.055 (0.055)\tData 0.002 (0.003)\tLoss 0.1164 (0.1741)\tPrec 95.312% (93.825%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.137 (0.137)\tLoss 0.4599 (0.4599)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.570% \n",
      "best acc: 88.600000\n",
      "Epoch: [93][0/391]\tTime 0.192 (0.192)\tData 0.131 (0.131)\tLoss 0.1171 (0.1171)\tPrec 95.312% (95.312%)\n",
      "Epoch: [93][100/391]\tTime 0.068 (0.066)\tData 0.002 (0.004)\tLoss 0.1534 (0.1648)\tPrec 94.531% (94.245%)\n",
      "Epoch: [93][200/391]\tTime 0.057 (0.066)\tData 0.002 (0.003)\tLoss 0.1605 (0.1737)\tPrec 94.531% (93.855%)\n",
      "Epoch: [93][300/391]\tTime 0.060 (0.063)\tData 0.002 (0.003)\tLoss 0.1631 (0.1807)\tPrec 95.312% (93.558%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.4816 (0.4816)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.630% \n",
      "best acc: 88.600000\n",
      "Epoch: [94][0/391]\tTime 0.177 (0.177)\tData 0.122 (0.122)\tLoss 0.1617 (0.1617)\tPrec 96.094% (96.094%)\n",
      "Epoch: [94][100/391]\tTime 0.062 (0.063)\tData 0.003 (0.004)\tLoss 0.1201 (0.1677)\tPrec 94.531% (94.067%)\n",
      "Epoch: [94][200/391]\tTime 0.069 (0.064)\tData 0.002 (0.003)\tLoss 0.1223 (0.1678)\tPrec 96.875% (94.034%)\n",
      "Epoch: [94][300/391]\tTime 0.061 (0.063)\tData 0.003 (0.003)\tLoss 0.1616 (0.1750)\tPrec 95.312% (93.859%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.3305 (0.3305)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.800% \n",
      "best acc: 88.600000\n",
      "Epoch: [95][0/391]\tTime 0.189 (0.189)\tData 0.123 (0.123)\tLoss 0.2169 (0.2169)\tPrec 89.062% (89.062%)\n",
      "Epoch: [95][100/391]\tTime 0.061 (0.065)\tData 0.002 (0.004)\tLoss 0.1628 (0.1786)\tPrec 93.750% (93.417%)\n",
      "Epoch: [95][200/391]\tTime 0.056 (0.066)\tData 0.002 (0.003)\tLoss 0.1538 (0.1766)\tPrec 93.750% (93.637%)\n",
      "Epoch: [95][300/391]\tTime 0.067 (0.065)\tData 0.002 (0.003)\tLoss 0.1715 (0.1783)\tPrec 95.312% (93.628%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.127 (0.127)\tLoss 0.3577 (0.3577)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.700% \n",
      "best acc: 88.600000\n",
      "Epoch: [96][0/391]\tTime 0.178 (0.178)\tData 0.114 (0.114)\tLoss 0.0961 (0.0961)\tPrec 97.656% (97.656%)\n",
      "Epoch: [96][100/391]\tTime 0.057 (0.061)\tData 0.002 (0.003)\tLoss 0.1823 (0.1663)\tPrec 92.969% (94.152%)\n",
      "Epoch: [96][200/391]\tTime 0.058 (0.060)\tData 0.002 (0.003)\tLoss 0.1597 (0.1707)\tPrec 94.531% (93.870%)\n",
      "Epoch: [96][300/391]\tTime 0.059 (0.060)\tData 0.002 (0.002)\tLoss 0.2484 (0.1745)\tPrec 90.625% (93.706%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.2838 (0.2838)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.550% \n",
      "best acc: 88.600000\n",
      "Epoch: [97][0/391]\tTime 0.182 (0.182)\tData 0.125 (0.125)\tLoss 0.1900 (0.1900)\tPrec 93.750% (93.750%)\n",
      "Epoch: [97][100/391]\tTime 0.056 (0.054)\tData 0.002 (0.003)\tLoss 0.2497 (0.1680)\tPrec 90.625% (94.106%)\n",
      "Epoch: [97][200/391]\tTime 0.056 (0.056)\tData 0.002 (0.003)\tLoss 0.1553 (0.1779)\tPrec 94.531% (93.777%)\n",
      "Epoch: [97][300/391]\tTime 0.058 (0.057)\tData 0.002 (0.002)\tLoss 0.3167 (0.1820)\tPrec 87.500% (93.638%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.1986 (0.1986)\tPrec 94.531% (94.531%)\n",
      " * Prec 87.260% \n",
      "best acc: 88.600000\n",
      "Epoch: [98][0/391]\tTime 0.196 (0.196)\tData 0.125 (0.125)\tLoss 0.1389 (0.1389)\tPrec 95.312% (95.312%)\n",
      "Epoch: [98][100/391]\tTime 0.061 (0.060)\tData 0.002 (0.003)\tLoss 0.1245 (0.1619)\tPrec 96.094% (94.322%)\n",
      "Epoch: [98][200/391]\tTime 0.069 (0.061)\tData 0.002 (0.003)\tLoss 0.0862 (0.1711)\tPrec 97.656% (93.890%)\n",
      "Epoch: [98][300/391]\tTime 0.068 (0.063)\tData 0.002 (0.003)\tLoss 0.3543 (0.1739)\tPrec 89.062% (93.846%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.3267 (0.3267)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.310% \n",
      "best acc: 88.600000\n",
      "Epoch: [99][0/391]\tTime 0.187 (0.187)\tData 0.124 (0.124)\tLoss 0.1410 (0.1410)\tPrec 96.094% (96.094%)\n",
      "Epoch: [99][100/391]\tTime 0.053 (0.059)\tData 0.002 (0.003)\tLoss 0.1637 (0.1703)\tPrec 94.531% (94.083%)\n",
      "Epoch: [99][200/391]\tTime 0.072 (0.062)\tData 0.002 (0.003)\tLoss 0.1299 (0.1773)\tPrec 96.094% (93.789%)\n",
      "Epoch: [99][300/391]\tTime 0.060 (0.062)\tData 0.002 (0.003)\tLoss 0.1077 (0.1806)\tPrec 95.312% (93.672%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3271 (0.3271)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.910% \n",
      "best acc: 88.600000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 5e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8860/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/resnet20_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f6cd222-0b1c-48b5-afef-b6d35b2b4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity level:  tensor(0.7999, device='cuda:0')\n",
      "Sparsity level:  tensor(0.7999, device='cuda:0')\n",
      "Sparsity level:  tensor(0.7999, device='cuda:0')\n",
      "Sparsity level:  tensor(0.7999, device='cuda:0')\n",
      "Sparsity level:  tensor(0.7999, device='cuda:0')\n",
      "Sparsity level:  tensor(0.7999, device='cuda:0')\n",
      "Sparsity level:  tensor(0.7999, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8008, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.7998, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n",
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#### Prune all the QuantConv2D layers' 80% weights with 1) unstructured, and 2) structured manner.\n",
    "############################################################\n",
    "####################### UNSTRUCTURED #######################\n",
    "############################################################\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        prune.l1_unstructured(layer, name='weight', amount=0.8)\n",
    "        ### Check sparsity ###\n",
    "        mask = layer.weight_mask\n",
    "        sparsity_mask = (mask == 0).sum() / mask.nelement()\n",
    "        print(\"Sparsity level: \", sparsity_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02311c2e-e4b4-4a24-be03-2a1c4e4222fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 938/10000 (9%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check accuracy after pruning\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9d9d470-95b1-43cc-8be0-aaafc8b841df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.218 (0.218)\tData 0.142 (0.142)\tLoss 1.3923 (1.3923)\tPrec 55.469% (55.469%)\n",
      "Epoch: [0][100/391]\tTime 0.050 (0.064)\tData 0.002 (0.004)\tLoss 0.5035 (0.6447)\tPrec 82.812% (78.086%)\n",
      "Epoch: [0][200/391]\tTime 0.062 (0.062)\tData 0.002 (0.003)\tLoss 0.3088 (0.5585)\tPrec 86.719% (80.838%)\n",
      "Epoch: [0][300/391]\tTime 0.062 (0.061)\tData 0.002 (0.003)\tLoss 0.4094 (0.5193)\tPrec 85.938% (82.062%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.134 (0.134)\tLoss 0.4479 (0.4479)\tPrec 85.938% (85.938%)\n",
      " * Prec 82.740% \n",
      "best acc: 82.740000\n",
      "Epoch: [1][0/391]\tTime 0.195 (0.195)\tData 0.131 (0.131)\tLoss 0.4090 (0.4090)\tPrec 82.031% (82.031%)\n",
      "Epoch: [1][100/391]\tTime 0.066 (0.065)\tData 0.002 (0.004)\tLoss 0.3155 (0.3754)\tPrec 90.625% (86.610%)\n",
      "Epoch: [1][200/391]\tTime 0.068 (0.063)\tData 0.002 (0.003)\tLoss 0.5445 (0.3744)\tPrec 85.156% (86.831%)\n",
      "Epoch: [1][300/391]\tTime 0.056 (0.059)\tData 0.002 (0.003)\tLoss 0.2178 (0.3659)\tPrec 92.969% (87.147%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.3427 (0.3427)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.600% \n",
      "best acc: 85.600000\n",
      "Epoch: [2][0/391]\tTime 0.184 (0.184)\tData 0.119 (0.119)\tLoss 0.2397 (0.2397)\tPrec 90.625% (90.625%)\n",
      "Epoch: [2][100/391]\tTime 0.060 (0.064)\tData 0.002 (0.003)\tLoss 0.3246 (0.3336)\tPrec 88.281% (88.134%)\n",
      "Epoch: [2][200/391]\tTime 0.059 (0.064)\tData 0.002 (0.003)\tLoss 0.4164 (0.3290)\tPrec 87.500% (88.351%)\n",
      "Epoch: [2][300/391]\tTime 0.064 (0.063)\tData 0.002 (0.003)\tLoss 0.3630 (0.3266)\tPrec 90.625% (88.551%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.4031 (0.4031)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.600% \n",
      "best acc: 85.600000\n",
      "Epoch: [3][0/391]\tTime 0.201 (0.201)\tData 0.138 (0.138)\tLoss 0.2434 (0.2434)\tPrec 91.406% (91.406%)\n",
      "Epoch: [3][100/391]\tTime 0.074 (0.069)\tData 0.002 (0.003)\tLoss 0.3004 (0.3099)\tPrec 87.500% (88.900%)\n",
      "Epoch: [3][200/391]\tTime 0.062 (0.067)\tData 0.002 (0.003)\tLoss 0.2161 (0.3050)\tPrec 91.406% (89.195%)\n",
      "Epoch: [3][300/391]\tTime 0.062 (0.064)\tData 0.002 (0.003)\tLoss 0.2690 (0.3091)\tPrec 90.625% (89.179%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.134 (0.134)\tLoss 0.2724 (0.2724)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.970% \n",
      "best acc: 85.970000\n",
      "Epoch: [4][0/391]\tTime 0.193 (0.193)\tData 0.125 (0.125)\tLoss 0.2915 (0.2915)\tPrec 89.844% (89.844%)\n",
      "Epoch: [4][100/391]\tTime 0.065 (0.066)\tData 0.002 (0.003)\tLoss 0.3510 (0.2922)\tPrec 86.719% (89.658%)\n",
      "Epoch: [4][200/391]\tTime 0.067 (0.067)\tData 0.002 (0.003)\tLoss 0.2977 (0.2965)\tPrec 89.844% (89.440%)\n",
      "Epoch: [4][300/391]\tTime 0.064 (0.065)\tData 0.002 (0.003)\tLoss 0.3599 (0.2966)\tPrec 87.500% (89.561%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.3437 (0.3437)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.900% \n",
      "best acc: 85.970000\n",
      "Epoch: [5][0/391]\tTime 0.190 (0.190)\tData 0.129 (0.129)\tLoss 0.2337 (0.2337)\tPrec 90.625% (90.625%)\n",
      "Epoch: [5][100/391]\tTime 0.058 (0.060)\tData 0.002 (0.003)\tLoss 0.2016 (0.2759)\tPrec 93.750% (90.331%)\n",
      "Epoch: [5][200/391]\tTime 0.052 (0.059)\tData 0.002 (0.003)\tLoss 0.3138 (0.2826)\tPrec 89.844% (90.015%)\n",
      "Epoch: [5][300/391]\tTime 0.058 (0.058)\tData 0.002 (0.002)\tLoss 0.2626 (0.2855)\tPrec 91.406% (89.948%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.134 (0.134)\tLoss 0.3075 (0.3075)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.490% \n",
      "best acc: 85.970000\n",
      "Epoch: [6][0/391]\tTime 0.189 (0.189)\tData 0.132 (0.132)\tLoss 0.2676 (0.2676)\tPrec 88.281% (88.281%)\n",
      "Epoch: [6][100/391]\tTime 0.069 (0.064)\tData 0.002 (0.004)\tLoss 0.3092 (0.2842)\tPrec 89.844% (89.921%)\n",
      "Epoch: [6][200/391]\tTime 0.066 (0.064)\tData 0.002 (0.003)\tLoss 0.2694 (0.2834)\tPrec 91.406% (89.820%)\n",
      "Epoch: [6][300/391]\tTime 0.072 (0.064)\tData 0.002 (0.003)\tLoss 0.3548 (0.2809)\tPrec 87.500% (89.932%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.3194 (0.3194)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.920% \n",
      "best acc: 85.970000\n",
      "Epoch: [7][0/391]\tTime 0.180 (0.180)\tData 0.126 (0.126)\tLoss 0.2065 (0.2065)\tPrec 92.969% (92.969%)\n",
      "Epoch: [7][100/391]\tTime 0.064 (0.058)\tData 0.003 (0.003)\tLoss 0.2046 (0.2734)\tPrec 94.531% (90.393%)\n",
      "Epoch: [7][200/391]\tTime 0.059 (0.057)\tData 0.002 (0.003)\tLoss 0.2674 (0.2785)\tPrec 88.281% (90.112%)\n",
      "Epoch: [7][300/391]\tTime 0.072 (0.058)\tData 0.002 (0.003)\tLoss 0.2551 (0.2800)\tPrec 92.188% (90.049%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.3173 (0.3173)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.510% \n",
      "best acc: 85.970000\n",
      "Epoch: [8][0/391]\tTime 0.189 (0.189)\tData 0.130 (0.130)\tLoss 0.2500 (0.2500)\tPrec 91.406% (91.406%)\n",
      "Epoch: [8][100/391]\tTime 0.045 (0.052)\tData 0.001 (0.003)\tLoss 0.2015 (0.2691)\tPrec 92.969% (90.718%)\n",
      "Epoch: [8][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.1862 (0.2711)\tPrec 92.969% (90.508%)\n",
      "Epoch: [8][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.3682 (0.2713)\tPrec 85.938% (90.449%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.3324 (0.3324)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.900% \n",
      "best acc: 85.970000\n",
      "Epoch: [9][0/391]\tTime 0.181 (0.181)\tData 0.119 (0.119)\tLoss 0.2060 (0.2060)\tPrec 93.750% (93.750%)\n",
      "Epoch: [9][100/391]\tTime 0.057 (0.059)\tData 0.002 (0.003)\tLoss 0.2713 (0.2725)\tPrec 88.281% (90.362%)\n",
      "Epoch: [9][200/391]\tTime 0.066 (0.061)\tData 0.002 (0.003)\tLoss 0.3451 (0.2777)\tPrec 88.281% (90.302%)\n",
      "Epoch: [9][300/391]\tTime 0.062 (0.062)\tData 0.002 (0.003)\tLoss 0.3209 (0.2809)\tPrec 88.281% (90.057%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.2608 (0.2608)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.590% \n",
      "best acc: 85.970000\n",
      "Epoch: [10][0/391]\tTime 0.188 (0.188)\tData 0.132 (0.132)\tLoss 0.2478 (0.2478)\tPrec 90.625% (90.625%)\n",
      "Epoch: [10][100/391]\tTime 0.073 (0.067)\tData 0.002 (0.004)\tLoss 0.3175 (0.2768)\tPrec 87.500% (90.184%)\n",
      "Epoch: [10][200/391]\tTime 0.070 (0.066)\tData 0.002 (0.003)\tLoss 0.4208 (0.2855)\tPrec 85.938% (89.984%)\n",
      "Epoch: [10][300/391]\tTime 0.068 (0.066)\tData 0.002 (0.003)\tLoss 0.2518 (0.2860)\tPrec 90.625% (89.989%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.2928 (0.2928)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.010% \n",
      "best acc: 86.010000\n",
      "Epoch: [11][0/391]\tTime 0.182 (0.182)\tData 0.122 (0.122)\tLoss 0.2845 (0.2845)\tPrec 90.625% (90.625%)\n",
      "Epoch: [11][100/391]\tTime 0.063 (0.061)\tData 0.003 (0.003)\tLoss 0.1983 (0.2658)\tPrec 94.531% (90.733%)\n",
      "Epoch: [11][200/391]\tTime 0.057 (0.060)\tData 0.001 (0.003)\tLoss 0.2837 (0.2721)\tPrec 89.844% (90.485%)\n",
      "Epoch: [11][300/391]\tTime 0.050 (0.060)\tData 0.002 (0.003)\tLoss 0.2443 (0.2695)\tPrec 92.969% (90.532%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.3575 (0.3575)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.960% \n",
      "best acc: 86.010000\n",
      "Epoch: [12][0/391]\tTime 0.213 (0.213)\tData 0.145 (0.145)\tLoss 0.2247 (0.2247)\tPrec 89.844% (89.844%)\n",
      "Epoch: [12][100/391]\tTime 0.043 (0.062)\tData 0.002 (0.004)\tLoss 0.3072 (0.2657)\tPrec 86.719% (90.494%)\n",
      "Epoch: [12][200/391]\tTime 0.063 (0.060)\tData 0.003 (0.003)\tLoss 0.2913 (0.2661)\tPrec 89.844% (90.481%)\n",
      "Epoch: [12][300/391]\tTime 0.070 (0.061)\tData 0.002 (0.003)\tLoss 0.2711 (0.2731)\tPrec 89.062% (90.272%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.2252 (0.2252)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.770% \n",
      "best acc: 86.770000\n",
      "Epoch: [13][0/391]\tTime 0.194 (0.194)\tData 0.129 (0.129)\tLoss 0.3297 (0.3297)\tPrec 88.281% (88.281%)\n",
      "Epoch: [13][100/391]\tTime 0.071 (0.065)\tData 0.002 (0.004)\tLoss 0.2647 (0.2691)\tPrec 92.969% (90.439%)\n",
      "Epoch: [13][200/391]\tTime 0.052 (0.063)\tData 0.002 (0.003)\tLoss 0.2478 (0.2697)\tPrec 89.062% (90.353%)\n",
      "Epoch: [13][300/391]\tTime 0.064 (0.062)\tData 0.002 (0.003)\tLoss 0.1825 (0.2716)\tPrec 92.188% (90.316%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.3252 (0.3252)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.500% \n",
      "best acc: 86.770000\n",
      "Epoch: [14][0/391]\tTime 0.181 (0.181)\tData 0.120 (0.120)\tLoss 0.1904 (0.1904)\tPrec 92.969% (92.969%)\n",
      "Epoch: [14][100/391]\tTime 0.054 (0.058)\tData 0.002 (0.003)\tLoss 0.1774 (0.2582)\tPrec 93.750% (90.849%)\n",
      "Epoch: [14][200/391]\tTime 0.059 (0.056)\tData 0.002 (0.003)\tLoss 0.3560 (0.2626)\tPrec 88.281% (90.742%)\n",
      "Epoch: [14][300/391]\tTime 0.055 (0.056)\tData 0.002 (0.002)\tLoss 0.3950 (0.2670)\tPrec 86.719% (90.570%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.2937 (0.2937)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.750% \n",
      "best acc: 86.770000\n",
      "Epoch: [15][0/391]\tTime 0.174 (0.174)\tData 0.121 (0.121)\tLoss 0.2544 (0.2544)\tPrec 90.625% (90.625%)\n",
      "Epoch: [15][100/391]\tTime 0.064 (0.066)\tData 0.002 (0.003)\tLoss 0.2504 (0.2587)\tPrec 90.625% (90.880%)\n",
      "Epoch: [15][200/391]\tTime 0.070 (0.067)\tData 0.002 (0.003)\tLoss 0.2668 (0.2623)\tPrec 91.406% (90.687%)\n",
      "Epoch: [15][300/391]\tTime 0.058 (0.063)\tData 0.002 (0.002)\tLoss 0.2601 (0.2638)\tPrec 90.625% (90.604%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.3222 (0.3222)\tPrec 91.406% (91.406%)\n",
      " * Prec 85.400% \n",
      "best acc: 86.770000\n",
      "Epoch: [16][0/391]\tTime 0.174 (0.174)\tData 0.118 (0.118)\tLoss 0.3766 (0.3766)\tPrec 92.188% (92.188%)\n",
      "Epoch: [16][100/391]\tTime 0.077 (0.066)\tData 0.002 (0.004)\tLoss 0.2472 (0.2510)\tPrec 93.750% (91.035%)\n",
      "Epoch: [16][200/391]\tTime 0.063 (0.066)\tData 0.002 (0.003)\tLoss 0.2014 (0.2592)\tPrec 92.969% (90.800%)\n",
      "Epoch: [16][300/391]\tTime 0.071 (0.067)\tData 0.002 (0.003)\tLoss 0.2574 (0.2632)\tPrec 90.625% (90.708%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.2497 (0.2497)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.100% \n",
      "best acc: 86.770000\n",
      "Epoch: [17][0/391]\tTime 0.178 (0.178)\tData 0.120 (0.120)\tLoss 0.2949 (0.2949)\tPrec 89.062% (89.062%)\n",
      "Epoch: [17][100/391]\tTime 0.060 (0.062)\tData 0.002 (0.003)\tLoss 0.2132 (0.2477)\tPrec 89.844% (91.306%)\n",
      "Epoch: [17][200/391]\tTime 0.053 (0.058)\tData 0.002 (0.003)\tLoss 0.2978 (0.2486)\tPrec 89.062% (91.231%)\n",
      "Epoch: [17][300/391]\tTime 0.058 (0.056)\tData 0.002 (0.002)\tLoss 0.3772 (0.2504)\tPrec 82.812% (91.160%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.2552 (0.2552)\tPrec 91.406% (91.406%)\n",
      " * Prec 85.640% \n",
      "best acc: 86.770000\n",
      "Epoch: [18][0/391]\tTime 0.181 (0.181)\tData 0.121 (0.121)\tLoss 0.1551 (0.1551)\tPrec 94.531% (94.531%)\n",
      "Epoch: [18][100/391]\tTime 0.063 (0.062)\tData 0.002 (0.003)\tLoss 0.3548 (0.2550)\tPrec 86.719% (90.803%)\n",
      "Epoch: [18][200/391]\tTime 0.062 (0.061)\tData 0.002 (0.003)\tLoss 0.2094 (0.2578)\tPrec 92.188% (90.819%)\n",
      "Epoch: [18][300/391]\tTime 0.063 (0.062)\tData 0.002 (0.003)\tLoss 0.2174 (0.2592)\tPrec 93.750% (90.817%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.179 (0.179)\tLoss 0.2940 (0.2940)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.600% \n",
      "best acc: 86.770000\n",
      "Epoch: [19][0/391]\tTime 0.177 (0.177)\tData 0.129 (0.129)\tLoss 0.1871 (0.1871)\tPrec 93.750% (93.750%)\n",
      "Epoch: [19][100/391]\tTime 0.059 (0.061)\tData 0.002 (0.003)\tLoss 0.2372 (0.2504)\tPrec 90.625% (91.012%)\n",
      "Epoch: [19][200/391]\tTime 0.063 (0.059)\tData 0.002 (0.003)\tLoss 0.2463 (0.2559)\tPrec 90.625% (90.835%)\n",
      "Epoch: [19][300/391]\tTime 0.053 (0.058)\tData 0.002 (0.003)\tLoss 0.2597 (0.2552)\tPrec 90.625% (90.877%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.3349 (0.3349)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.350% \n",
      "best acc: 86.770000\n",
      "Epoch: [20][0/391]\tTime 0.187 (0.187)\tData 0.123 (0.123)\tLoss 0.1387 (0.1387)\tPrec 95.312% (95.312%)\n",
      "Epoch: [20][100/391]\tTime 0.046 (0.052)\tData 0.002 (0.003)\tLoss 0.1985 (0.2516)\tPrec 92.188% (91.136%)\n",
      "Epoch: [20][200/391]\tTime 0.061 (0.056)\tData 0.002 (0.003)\tLoss 0.3417 (0.2602)\tPrec 88.281% (90.948%)\n",
      "Epoch: [20][300/391]\tTime 0.058 (0.057)\tData 0.002 (0.003)\tLoss 0.1771 (0.2606)\tPrec 96.094% (90.848%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.4345 (0.4345)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.360% \n",
      "best acc: 86.770000\n",
      "Epoch: [21][0/391]\tTime 0.180 (0.180)\tData 0.120 (0.120)\tLoss 0.2566 (0.2566)\tPrec 90.625% (90.625%)\n",
      "Epoch: [21][100/391]\tTime 0.063 (0.064)\tData 0.002 (0.003)\tLoss 0.2330 (0.2449)\tPrec 90.625% (91.267%)\n",
      "Epoch: [21][200/391]\tTime 0.064 (0.063)\tData 0.003 (0.003)\tLoss 0.3750 (0.2488)\tPrec 85.156% (91.084%)\n",
      "Epoch: [21][300/391]\tTime 0.063 (0.060)\tData 0.003 (0.003)\tLoss 0.2102 (0.2502)\tPrec 93.750% (91.167%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.3824 (0.3824)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.630% \n",
      "best acc: 86.770000\n",
      "Epoch: [22][0/391]\tTime 0.176 (0.176)\tData 0.118 (0.118)\tLoss 0.2316 (0.2316)\tPrec 92.188% (92.188%)\n",
      "Epoch: [22][100/391]\tTime 0.061 (0.060)\tData 0.002 (0.003)\tLoss 0.1675 (0.2467)\tPrec 96.875% (91.329%)\n",
      "Epoch: [22][200/391]\tTime 0.064 (0.061)\tData 0.002 (0.003)\tLoss 0.3097 (0.2481)\tPrec 88.281% (91.204%)\n",
      "Epoch: [22][300/391]\tTime 0.050 (0.061)\tData 0.002 (0.003)\tLoss 0.2152 (0.2520)\tPrec 92.188% (91.038%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.3128 (0.3128)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.780% \n",
      "best acc: 86.780000\n",
      "Epoch: [23][0/391]\tTime 0.183 (0.183)\tData 0.123 (0.123)\tLoss 0.3118 (0.3118)\tPrec 89.844% (89.844%)\n",
      "Epoch: [23][100/391]\tTime 0.051 (0.058)\tData 0.002 (0.003)\tLoss 0.2374 (0.2568)\tPrec 89.062% (90.834%)\n",
      "Epoch: [23][200/391]\tTime 0.048 (0.055)\tData 0.002 (0.003)\tLoss 0.3095 (0.2535)\tPrec 89.062% (90.963%)\n",
      "Epoch: [23][300/391]\tTime 0.045 (0.054)\tData 0.002 (0.002)\tLoss 0.2697 (0.2547)\tPrec 90.625% (90.957%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.2519 (0.2519)\tPrec 91.406% (91.406%)\n",
      " * Prec 85.970% \n",
      "best acc: 86.780000\n",
      "Epoch: [24][0/391]\tTime 0.179 (0.179)\tData 0.122 (0.122)\tLoss 0.2773 (0.2773)\tPrec 89.844% (89.844%)\n",
      "Epoch: [24][100/391]\tTime 0.069 (0.064)\tData 0.002 (0.003)\tLoss 0.2078 (0.2435)\tPrec 93.750% (91.445%)\n",
      "Epoch: [24][200/391]\tTime 0.057 (0.063)\tData 0.002 (0.003)\tLoss 0.3119 (0.2459)\tPrec 88.281% (91.255%)\n",
      "Epoch: [24][300/391]\tTime 0.063 (0.063)\tData 0.002 (0.003)\tLoss 0.2166 (0.2506)\tPrec 92.969% (91.160%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.2690 (0.2690)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.180% \n",
      "best acc: 86.780000\n",
      "Epoch: [25][0/391]\tTime 0.191 (0.191)\tData 0.126 (0.126)\tLoss 0.2232 (0.2232)\tPrec 92.188% (92.188%)\n",
      "Epoch: [25][100/391]\tTime 0.052 (0.054)\tData 0.002 (0.003)\tLoss 0.2154 (0.2477)\tPrec 93.750% (91.213%)\n",
      "Epoch: [25][200/391]\tTime 0.053 (0.054)\tData 0.002 (0.003)\tLoss 0.3401 (0.2451)\tPrec 87.500% (91.410%)\n",
      "Epoch: [25][300/391]\tTime 0.049 (0.056)\tData 0.002 (0.002)\tLoss 0.2798 (0.2492)\tPrec 89.844% (91.266%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.144 (0.144)\tLoss 0.4169 (0.4169)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.530% \n",
      "best acc: 86.780000\n",
      "Epoch: [26][0/391]\tTime 0.181 (0.181)\tData 0.116 (0.116)\tLoss 0.2266 (0.2266)\tPrec 91.406% (91.406%)\n",
      "Epoch: [26][100/391]\tTime 0.057 (0.056)\tData 0.002 (0.003)\tLoss 0.2422 (0.2513)\tPrec 89.062% (91.066%)\n",
      "Epoch: [26][200/391]\tTime 0.062 (0.059)\tData 0.002 (0.003)\tLoss 0.1869 (0.2495)\tPrec 96.094% (91.103%)\n",
      "Epoch: [26][300/391]\tTime 0.063 (0.059)\tData 0.003 (0.003)\tLoss 0.2464 (0.2560)\tPrec 92.188% (90.827%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.3535 (0.3535)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.660% \n",
      "best acc: 86.780000\n",
      "Epoch: [27][0/391]\tTime 0.184 (0.184)\tData 0.125 (0.125)\tLoss 0.1765 (0.1765)\tPrec 93.750% (93.750%)\n",
      "Epoch: [27][100/391]\tTime 0.069 (0.068)\tData 0.002 (0.003)\tLoss 0.2594 (0.2474)\tPrec 89.844% (91.399%)\n",
      "Epoch: [27][200/391]\tTime 0.062 (0.064)\tData 0.002 (0.003)\tLoss 0.2567 (0.2501)\tPrec 90.625% (91.181%)\n",
      "Epoch: [27][300/391]\tTime 0.060 (0.063)\tData 0.002 (0.003)\tLoss 0.2374 (0.2512)\tPrec 91.406% (91.082%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.3256 (0.3256)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.040% \n",
      "best acc: 86.780000\n",
      "Epoch: [28][0/391]\tTime 0.194 (0.194)\tData 0.130 (0.130)\tLoss 0.3259 (0.3259)\tPrec 84.375% (84.375%)\n",
      "Epoch: [28][100/391]\tTime 0.058 (0.057)\tData 0.002 (0.003)\tLoss 0.3327 (0.2444)\tPrec 89.844% (91.437%)\n",
      "Epoch: [28][200/391]\tTime 0.062 (0.057)\tData 0.002 (0.003)\tLoss 0.2430 (0.2475)\tPrec 92.188% (91.371%)\n",
      "Epoch: [28][300/391]\tTime 0.056 (0.058)\tData 0.002 (0.003)\tLoss 0.2304 (0.2481)\tPrec 90.625% (91.313%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.2690 (0.2690)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.220% \n",
      "best acc: 86.780000\n",
      "Epoch: [29][0/391]\tTime 0.178 (0.178)\tData 0.115 (0.115)\tLoss 0.2051 (0.2051)\tPrec 94.531% (94.531%)\n",
      "Epoch: [29][100/391]\tTime 0.058 (0.057)\tData 0.002 (0.003)\tLoss 0.3279 (0.2585)\tPrec 89.062% (91.058%)\n",
      "Epoch: [29][200/391]\tTime 0.061 (0.057)\tData 0.002 (0.003)\tLoss 0.2516 (0.2515)\tPrec 91.406% (91.243%)\n",
      "Epoch: [29][300/391]\tTime 0.051 (0.058)\tData 0.002 (0.003)\tLoss 0.2672 (0.2512)\tPrec 90.625% (91.178%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.4136 (0.4136)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.810% \n",
      "best acc: 86.780000\n",
      "Epoch: [30][0/391]\tTime 0.192 (0.192)\tData 0.137 (0.137)\tLoss 0.2109 (0.2109)\tPrec 92.969% (92.969%)\n",
      "Epoch: [30][100/391]\tTime 0.060 (0.054)\tData 0.002 (0.003)\tLoss 0.2139 (0.2441)\tPrec 92.188% (91.360%)\n",
      "Epoch: [30][200/391]\tTime 0.062 (0.057)\tData 0.002 (0.003)\tLoss 0.2019 (0.2486)\tPrec 93.750% (91.146%)\n",
      "Epoch: [30][300/391]\tTime 0.062 (0.058)\tData 0.002 (0.003)\tLoss 0.2234 (0.2524)\tPrec 93.750% (90.996%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.4251 (0.4251)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.180% \n",
      "best acc: 86.780000\n",
      "Epoch: [31][0/391]\tTime 0.185 (0.185)\tData 0.124 (0.124)\tLoss 0.2931 (0.2931)\tPrec 89.844% (89.844%)\n",
      "Epoch: [31][100/391]\tTime 0.055 (0.062)\tData 0.002 (0.003)\tLoss 0.2524 (0.2458)\tPrec 88.281% (91.097%)\n",
      "Epoch: [31][200/391]\tTime 0.064 (0.062)\tData 0.002 (0.003)\tLoss 0.3604 (0.2481)\tPrec 87.500% (91.146%)\n",
      "Epoch: [31][300/391]\tTime 0.064 (0.062)\tData 0.002 (0.003)\tLoss 0.2273 (0.2511)\tPrec 91.406% (91.053%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.3012 (0.3012)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.880% \n",
      "best acc: 86.880000\n",
      "Epoch: [32][0/391]\tTime 0.169 (0.169)\tData 0.117 (0.117)\tLoss 0.2137 (0.2137)\tPrec 93.750% (93.750%)\n",
      "Epoch: [32][100/391]\tTime 0.058 (0.056)\tData 0.002 (0.003)\tLoss 0.1880 (0.2382)\tPrec 93.750% (91.522%)\n",
      "Epoch: [32][200/391]\tTime 0.058 (0.058)\tData 0.002 (0.003)\tLoss 0.2605 (0.2410)\tPrec 89.062% (91.383%)\n",
      "Epoch: [32][300/391]\tTime 0.061 (0.059)\tData 0.002 (0.003)\tLoss 0.3891 (0.2478)\tPrec 85.156% (91.175%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.2982 (0.2982)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.080% \n",
      "best acc: 87.080000\n",
      "Epoch: [33][0/391]\tTime 0.178 (0.178)\tData 0.117 (0.117)\tLoss 0.1948 (0.1948)\tPrec 93.750% (93.750%)\n",
      "Epoch: [33][100/391]\tTime 0.062 (0.060)\tData 0.002 (0.003)\tLoss 0.2559 (0.2401)\tPrec 89.844% (91.437%)\n",
      "Epoch: [33][200/391]\tTime 0.056 (0.060)\tData 0.002 (0.003)\tLoss 0.1387 (0.2417)\tPrec 96.094% (91.535%)\n",
      "Epoch: [33][300/391]\tTime 0.063 (0.059)\tData 0.002 (0.003)\tLoss 0.2278 (0.2434)\tPrec 92.969% (91.502%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3123 (0.3123)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.970% \n",
      "best acc: 87.080000\n",
      "Epoch: [34][0/391]\tTime 0.187 (0.187)\tData 0.126 (0.126)\tLoss 0.3232 (0.3232)\tPrec 89.844% (89.844%)\n",
      "Epoch: [34][100/391]\tTime 0.063 (0.064)\tData 0.002 (0.004)\tLoss 0.2285 (0.2508)\tPrec 91.406% (91.275%)\n",
      "Epoch: [34][200/391]\tTime 0.061 (0.062)\tData 0.002 (0.003)\tLoss 0.1925 (0.2506)\tPrec 92.188% (91.103%)\n",
      "Epoch: [34][300/391]\tTime 0.059 (0.060)\tData 0.002 (0.003)\tLoss 0.2873 (0.2512)\tPrec 89.062% (91.087%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.2653 (0.2653)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.180% \n",
      "best acc: 87.080000\n",
      "Epoch: [35][0/391]\tTime 0.168 (0.168)\tData 0.119 (0.119)\tLoss 0.1896 (0.1896)\tPrec 93.750% (93.750%)\n",
      "Epoch: [35][100/391]\tTime 0.060 (0.058)\tData 0.002 (0.003)\tLoss 0.2276 (0.2343)\tPrec 92.188% (91.530%)\n",
      "Epoch: [35][200/391]\tTime 0.063 (0.057)\tData 0.003 (0.003)\tLoss 0.1955 (0.2417)\tPrec 91.406% (91.251%)\n",
      "Epoch: [35][300/391]\tTime 0.064 (0.058)\tData 0.002 (0.003)\tLoss 0.2405 (0.2455)\tPrec 93.750% (91.103%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.3195 (0.3195)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.010% \n",
      "best acc: 87.080000\n",
      "Epoch: [36][0/391]\tTime 0.180 (0.180)\tData 0.133 (0.133)\tLoss 0.2428 (0.2428)\tPrec 92.969% (92.969%)\n",
      "Epoch: [36][100/391]\tTime 0.052 (0.060)\tData 0.002 (0.004)\tLoss 0.1638 (0.2427)\tPrec 93.750% (91.298%)\n",
      "Epoch: [36][200/391]\tTime 0.068 (0.059)\tData 0.003 (0.003)\tLoss 0.1946 (0.2452)\tPrec 94.531% (91.286%)\n",
      "Epoch: [36][300/391]\tTime 0.065 (0.059)\tData 0.003 (0.003)\tLoss 0.2440 (0.2454)\tPrec 92.188% (91.243%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.3027 (0.3027)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.060% \n",
      "best acc: 87.080000\n",
      "Epoch: [37][0/391]\tTime 0.191 (0.191)\tData 0.140 (0.140)\tLoss 0.2256 (0.2256)\tPrec 92.188% (92.188%)\n",
      "Epoch: [37][100/391]\tTime 0.075 (0.060)\tData 0.011 (0.004)\tLoss 0.2450 (0.2450)\tPrec 91.406% (91.468%)\n",
      "Epoch: [37][200/391]\tTime 0.066 (0.060)\tData 0.002 (0.003)\tLoss 0.2320 (0.2487)\tPrec 90.625% (91.196%)\n",
      "Epoch: [37][300/391]\tTime 0.052 (0.060)\tData 0.002 (0.003)\tLoss 0.2254 (0.2495)\tPrec 91.406% (91.121%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.3240 (0.3240)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.740% \n",
      "best acc: 87.080000\n",
      "Epoch: [38][0/391]\tTime 0.174 (0.174)\tData 0.123 (0.123)\tLoss 0.2065 (0.2065)\tPrec 92.188% (92.188%)\n",
      "Epoch: [38][100/391]\tTime 0.049 (0.055)\tData 0.002 (0.003)\tLoss 0.2478 (0.2381)\tPrec 89.062% (91.375%)\n",
      "Epoch: [38][200/391]\tTime 0.056 (0.052)\tData 0.003 (0.003)\tLoss 0.2021 (0.2459)\tPrec 92.188% (91.301%)\n",
      "Epoch: [38][300/391]\tTime 0.044 (0.054)\tData 0.002 (0.003)\tLoss 0.3307 (0.2499)\tPrec 91.406% (91.183%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.3678 (0.3678)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.680% \n",
      "best acc: 87.080000\n",
      "Epoch: [39][0/391]\tTime 0.186 (0.186)\tData 0.130 (0.130)\tLoss 0.2054 (0.2054)\tPrec 95.312% (95.312%)\n",
      "Epoch: [39][100/391]\tTime 0.050 (0.058)\tData 0.002 (0.004)\tLoss 0.3316 (0.2510)\tPrec 88.281% (91.228%)\n",
      "Epoch: [39][200/391]\tTime 0.066 (0.060)\tData 0.002 (0.003)\tLoss 0.2292 (0.2465)\tPrec 92.188% (91.255%)\n",
      "Epoch: [39][300/391]\tTime 0.063 (0.059)\tData 0.003 (0.003)\tLoss 0.2399 (0.2433)\tPrec 89.062% (91.323%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.3549 (0.3549)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.590% \n",
      "best acc: 87.080000\n",
      "Epoch: [40][0/391]\tTime 0.181 (0.181)\tData 0.129 (0.129)\tLoss 0.1818 (0.1818)\tPrec 93.750% (93.750%)\n",
      "Epoch: [40][100/391]\tTime 0.051 (0.057)\tData 0.002 (0.003)\tLoss 0.2976 (0.2477)\tPrec 89.062% (91.105%)\n",
      "Epoch: [40][200/391]\tTime 0.056 (0.056)\tData 0.003 (0.003)\tLoss 0.1968 (0.2413)\tPrec 93.750% (91.356%)\n",
      "Epoch: [40][300/391]\tTime 0.053 (0.057)\tData 0.002 (0.003)\tLoss 0.2134 (0.2456)\tPrec 91.406% (91.230%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.2829 (0.2829)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.850% \n",
      "best acc: 87.080000\n",
      "Epoch: [41][0/391]\tTime 0.183 (0.183)\tData 0.116 (0.116)\tLoss 0.1859 (0.1859)\tPrec 92.188% (92.188%)\n",
      "Epoch: [41][100/391]\tTime 0.049 (0.056)\tData 0.002 (0.003)\tLoss 0.3554 (0.2455)\tPrec 87.500% (91.391%)\n",
      "Epoch: [41][200/391]\tTime 0.059 (0.055)\tData 0.003 (0.003)\tLoss 0.3025 (0.2439)\tPrec 89.844% (91.437%)\n",
      "Epoch: [41][300/391]\tTime 0.061 (0.056)\tData 0.002 (0.003)\tLoss 0.2022 (0.2442)\tPrec 91.406% (91.463%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.162 (0.162)\tLoss 0.4016 (0.4016)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.320% \n",
      "best acc: 87.080000\n",
      "Epoch: [42][0/391]\tTime 0.173 (0.173)\tData 0.122 (0.122)\tLoss 0.1754 (0.1754)\tPrec 94.531% (94.531%)\n",
      "Epoch: [42][100/391]\tTime 0.049 (0.058)\tData 0.002 (0.003)\tLoss 0.1347 (0.2470)\tPrec 95.312% (91.089%)\n",
      "Epoch: [42][200/391]\tTime 0.058 (0.057)\tData 0.003 (0.003)\tLoss 0.2912 (0.2445)\tPrec 89.062% (91.332%)\n",
      "Epoch: [42][300/391]\tTime 0.061 (0.059)\tData 0.002 (0.003)\tLoss 0.3964 (0.2423)\tPrec 85.156% (91.383%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 0.2927 (0.2927)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.900% \n",
      "best acc: 87.080000\n",
      "Epoch: [43][0/391]\tTime 0.173 (0.173)\tData 0.118 (0.118)\tLoss 0.2761 (0.2761)\tPrec 90.625% (90.625%)\n",
      "Epoch: [43][100/391]\tTime 0.060 (0.058)\tData 0.004 (0.003)\tLoss 0.2984 (0.2479)\tPrec 92.188% (91.174%)\n",
      "Epoch: [43][200/391]\tTime 0.062 (0.057)\tData 0.003 (0.003)\tLoss 0.2477 (0.2463)\tPrec 90.625% (91.196%)\n",
      "Epoch: [43][300/391]\tTime 0.060 (0.055)\tData 0.003 (0.003)\tLoss 0.1976 (0.2499)\tPrec 91.406% (91.066%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.2682 (0.2682)\tPrec 92.188% (92.188%)\n",
      " * Prec 86.460% \n",
      "best acc: 87.080000\n",
      "Epoch: [44][0/391]\tTime 0.175 (0.175)\tData 0.123 (0.123)\tLoss 0.1431 (0.1431)\tPrec 92.969% (92.969%)\n",
      "Epoch: [44][100/391]\tTime 0.052 (0.056)\tData 0.002 (0.003)\tLoss 0.4055 (0.2377)\tPrec 84.375% (91.731%)\n",
      "Epoch: [44][200/391]\tTime 0.060 (0.058)\tData 0.002 (0.003)\tLoss 0.1650 (0.2444)\tPrec 92.969% (91.387%)\n",
      "Epoch: [44][300/391]\tTime 0.061 (0.059)\tData 0.002 (0.003)\tLoss 0.3135 (0.2471)\tPrec 88.281% (91.258%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.2991 (0.2991)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.900% \n",
      "best acc: 87.080000\n",
      "Epoch: [45][0/391]\tTime 0.181 (0.181)\tData 0.120 (0.120)\tLoss 0.3082 (0.3082)\tPrec 88.281% (88.281%)\n",
      "Epoch: [45][100/391]\tTime 0.052 (0.060)\tData 0.002 (0.003)\tLoss 0.2735 (0.2498)\tPrec 89.844% (91.313%)\n",
      "Epoch: [45][200/391]\tTime 0.044 (0.059)\tData 0.002 (0.003)\tLoss 0.3085 (0.2497)\tPrec 86.719% (91.309%)\n",
      "Epoch: [45][300/391]\tTime 0.062 (0.057)\tData 0.002 (0.003)\tLoss 0.2363 (0.2464)\tPrec 91.406% (91.385%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.2310 (0.2310)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.310% \n",
      "best acc: 87.080000\n",
      "Epoch: [46][0/391]\tTime 0.163 (0.163)\tData 0.118 (0.118)\tLoss 0.3217 (0.3217)\tPrec 89.844% (89.844%)\n",
      "Epoch: [46][100/391]\tTime 0.054 (0.055)\tData 0.002 (0.003)\tLoss 0.2641 (0.2361)\tPrec 92.969% (91.754%)\n",
      "Epoch: [46][200/391]\tTime 0.056 (0.058)\tData 0.002 (0.003)\tLoss 0.1736 (0.2452)\tPrec 93.750% (91.177%)\n",
      "Epoch: [46][300/391]\tTime 0.063 (0.058)\tData 0.003 (0.003)\tLoss 0.2983 (0.2485)\tPrec 91.406% (91.103%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.123 (0.123)\tLoss 0.2590 (0.2590)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.230% \n",
      "best acc: 87.230000\n",
      "Epoch: [47][0/391]\tTime 0.206 (0.206)\tData 0.143 (0.143)\tLoss 0.1324 (0.1324)\tPrec 96.094% (96.094%)\n",
      "Epoch: [47][100/391]\tTime 0.062 (0.057)\tData 0.003 (0.004)\tLoss 0.2935 (0.2354)\tPrec 89.844% (91.708%)\n",
      "Epoch: [47][200/391]\tTime 0.071 (0.058)\tData 0.008 (0.003)\tLoss 0.1794 (0.2408)\tPrec 93.750% (91.457%)\n",
      "Epoch: [47][300/391]\tTime 0.054 (0.060)\tData 0.002 (0.003)\tLoss 0.1818 (0.2434)\tPrec 94.531% (91.253%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.3301 (0.3301)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.970% \n",
      "best acc: 87.230000\n",
      "Epoch: [48][0/391]\tTime 0.180 (0.180)\tData 0.125 (0.125)\tLoss 0.2562 (0.2562)\tPrec 89.844% (89.844%)\n",
      "Epoch: [48][100/391]\tTime 0.058 (0.062)\tData 0.002 (0.004)\tLoss 0.2786 (0.2422)\tPrec 92.188% (91.406%)\n",
      "Epoch: [48][200/391]\tTime 0.060 (0.062)\tData 0.002 (0.003)\tLoss 0.2359 (0.2403)\tPrec 89.062% (91.406%)\n",
      "Epoch: [48][300/391]\tTime 0.066 (0.063)\tData 0.003 (0.003)\tLoss 0.3915 (0.2434)\tPrec 87.500% (91.315%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.4718 (0.4718)\tPrec 84.375% (84.375%)\n",
      " * Prec 86.330% \n",
      "best acc: 87.230000\n",
      "Epoch: [49][0/391]\tTime 0.187 (0.187)\tData 0.138 (0.138)\tLoss 0.2030 (0.2030)\tPrec 92.969% (92.969%)\n",
      "Epoch: [49][100/391]\tTime 0.055 (0.062)\tData 0.002 (0.004)\tLoss 0.2892 (0.2430)\tPrec 88.281% (91.460%)\n",
      "Epoch: [49][200/391]\tTime 0.050 (0.061)\tData 0.002 (0.003)\tLoss 0.2788 (0.2476)\tPrec 89.844% (91.305%)\n",
      "Epoch: [49][300/391]\tTime 0.052 (0.060)\tData 0.002 (0.003)\tLoss 0.1955 (0.2493)\tPrec 95.312% (91.313%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.3650 (0.3650)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.560% \n",
      "best acc: 87.230000\n",
      "Epoch: [50][0/391]\tTime 0.186 (0.186)\tData 0.119 (0.119)\tLoss 0.2492 (0.2492)\tPrec 92.969% (92.969%)\n",
      "Epoch: [50][100/391]\tTime 0.052 (0.055)\tData 0.002 (0.003)\tLoss 0.2012 (0.2370)\tPrec 91.406% (91.460%)\n",
      "Epoch: [50][200/391]\tTime 0.058 (0.055)\tData 0.002 (0.003)\tLoss 0.2142 (0.2409)\tPrec 94.531% (91.484%)\n",
      "Epoch: [50][300/391]\tTime 0.064 (0.057)\tData 0.003 (0.003)\tLoss 0.2997 (0.2393)\tPrec 90.625% (91.583%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.138 (0.138)\tLoss 0.2905 (0.2905)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.070% \n",
      "best acc: 87.230000\n",
      "Epoch: [51][0/391]\tTime 0.191 (0.191)\tData 0.120 (0.120)\tLoss 0.2319 (0.2319)\tPrec 89.844% (89.844%)\n",
      "Epoch: [51][100/391]\tTime 0.065 (0.060)\tData 0.003 (0.004)\tLoss 0.1714 (0.2394)\tPrec 94.531% (91.112%)\n",
      "Epoch: [51][200/391]\tTime 0.058 (0.057)\tData 0.002 (0.003)\tLoss 0.2340 (0.2459)\tPrec 90.625% (91.056%)\n",
      "Epoch: [51][300/391]\tTime 0.061 (0.057)\tData 0.003 (0.003)\tLoss 0.2133 (0.2458)\tPrec 95.312% (91.064%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.3602 (0.3602)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.750% \n",
      "best acc: 87.230000\n",
      "Epoch: [52][0/391]\tTime 0.188 (0.188)\tData 0.131 (0.131)\tLoss 0.3046 (0.3046)\tPrec 89.844% (89.844%)\n",
      "Epoch: [52][100/391]\tTime 0.056 (0.057)\tData 0.002 (0.003)\tLoss 0.3239 (0.2431)\tPrec 86.719% (91.267%)\n",
      "Epoch: [52][200/391]\tTime 0.062 (0.059)\tData 0.002 (0.003)\tLoss 0.2893 (0.2477)\tPrec 89.844% (91.185%)\n",
      "Epoch: [52][300/391]\tTime 0.059 (0.058)\tData 0.003 (0.003)\tLoss 0.2533 (0.2500)\tPrec 92.188% (91.056%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.2823 (0.2823)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.820% \n",
      "best acc: 87.230000\n",
      "Epoch: [53][0/391]\tTime 0.195 (0.195)\tData 0.130 (0.130)\tLoss 0.1951 (0.1951)\tPrec 92.969% (92.969%)\n",
      "Epoch: [53][100/391]\tTime 0.050 (0.062)\tData 0.002 (0.004)\tLoss 0.2384 (0.2350)\tPrec 90.625% (91.553%)\n",
      "Epoch: [53][200/391]\tTime 0.064 (0.060)\tData 0.002 (0.003)\tLoss 0.2611 (0.2377)\tPrec 89.062% (91.531%)\n",
      "Epoch: [53][300/391]\tTime 0.056 (0.060)\tData 0.002 (0.003)\tLoss 0.2250 (0.2389)\tPrec 92.969% (91.505%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.3038 (0.3038)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.350% \n",
      "best acc: 87.230000\n",
      "Epoch: [54][0/391]\tTime 0.164 (0.164)\tData 0.117 (0.117)\tLoss 0.2135 (0.2135)\tPrec 92.188% (92.188%)\n",
      "Epoch: [54][100/391]\tTime 0.052 (0.056)\tData 0.002 (0.003)\tLoss 0.2352 (0.2315)\tPrec 92.969% (91.677%)\n",
      "Epoch: [54][200/391]\tTime 0.064 (0.057)\tData 0.003 (0.003)\tLoss 0.2402 (0.2378)\tPrec 90.625% (91.585%)\n",
      "Epoch: [54][300/391]\tTime 0.065 (0.059)\tData 0.002 (0.003)\tLoss 0.2181 (0.2433)\tPrec 90.625% (91.409%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.138 (0.138)\tLoss 0.4050 (0.4050)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.890% \n",
      "best acc: 87.230000\n",
      "Epoch: [55][0/391]\tTime 0.183 (0.183)\tData 0.120 (0.120)\tLoss 0.2345 (0.2345)\tPrec 90.625% (90.625%)\n",
      "Epoch: [55][100/391]\tTime 0.063 (0.063)\tData 0.002 (0.004)\tLoss 0.1982 (0.2361)\tPrec 92.188% (91.484%)\n",
      "Epoch: [55][200/391]\tTime 0.053 (0.060)\tData 0.002 (0.003)\tLoss 0.1888 (0.2385)\tPrec 94.531% (91.562%)\n",
      "Epoch: [55][300/391]\tTime 0.062 (0.059)\tData 0.003 (0.003)\tLoss 0.3192 (0.2387)\tPrec 89.062% (91.507%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.2937 (0.2937)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.920% \n",
      "best acc: 87.230000\n",
      "Epoch: [56][0/391]\tTime 0.197 (0.197)\tData 0.138 (0.138)\tLoss 0.2741 (0.2741)\tPrec 89.844% (89.844%)\n",
      "Epoch: [56][100/391]\tTime 0.066 (0.064)\tData 0.002 (0.004)\tLoss 0.3308 (0.2410)\tPrec 88.281% (91.414%)\n",
      "Epoch: [56][200/391]\tTime 0.045 (0.058)\tData 0.002 (0.003)\tLoss 0.1953 (0.2416)\tPrec 92.188% (91.402%)\n",
      "Epoch: [56][300/391]\tTime 0.055 (0.057)\tData 0.002 (0.003)\tLoss 0.2731 (0.2429)\tPrec 85.156% (91.375%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.4035 (0.4035)\tPrec 82.812% (82.812%)\n",
      " * Prec 84.740% \n",
      "best acc: 87.230000\n",
      "Epoch: [57][0/391]\tTime 0.178 (0.178)\tData 0.125 (0.125)\tLoss 0.2102 (0.2102)\tPrec 91.406% (91.406%)\n",
      "Epoch: [57][100/391]\tTime 0.042 (0.047)\tData 0.002 (0.003)\tLoss 0.1693 (0.2318)\tPrec 94.531% (92.149%)\n",
      "Epoch: [57][200/391]\tTime 0.055 (0.049)\tData 0.002 (0.003)\tLoss 0.2915 (0.2388)\tPrec 89.844% (91.709%)\n",
      "Epoch: [57][300/391]\tTime 0.051 (0.052)\tData 0.002 (0.003)\tLoss 0.2644 (0.2428)\tPrec 92.188% (91.489%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.144 (0.144)\tLoss 0.2445 (0.2445)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.240% \n",
      "best acc: 87.230000\n",
      "Epoch: [58][0/391]\tTime 0.165 (0.165)\tData 0.115 (0.115)\tLoss 0.1430 (0.1430)\tPrec 95.312% (95.312%)\n",
      "Epoch: [58][100/391]\tTime 0.047 (0.058)\tData 0.002 (0.003)\tLoss 0.2749 (0.2366)\tPrec 90.625% (91.592%)\n",
      "Epoch: [58][200/391]\tTime 0.049 (0.056)\tData 0.002 (0.003)\tLoss 0.2790 (0.2444)\tPrec 89.062% (91.294%)\n",
      "Epoch: [58][300/391]\tTime 0.054 (0.055)\tData 0.002 (0.003)\tLoss 0.3484 (0.2449)\tPrec 91.406% (91.225%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.134 (0.134)\tLoss 0.3872 (0.3872)\tPrec 85.156% (85.156%)\n",
      " * Prec 86.280% \n",
      "best acc: 87.230000\n",
      "Epoch: [59][0/391]\tTime 0.208 (0.208)\tData 0.144 (0.144)\tLoss 0.1751 (0.1751)\tPrec 92.969% (92.969%)\n",
      "Epoch: [59][100/391]\tTime 0.066 (0.054)\tData 0.002 (0.004)\tLoss 0.2365 (0.2383)\tPrec 92.188% (91.576%)\n",
      "Epoch: [59][200/391]\tTime 0.051 (0.053)\tData 0.002 (0.003)\tLoss 0.4295 (0.2467)\tPrec 85.938% (91.426%)\n",
      "Epoch: [59][300/391]\tTime 0.045 (0.052)\tData 0.002 (0.003)\tLoss 0.2115 (0.2481)\tPrec 93.750% (91.248%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.3295 (0.3295)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.050% \n",
      "best acc: 87.230000\n",
      "Epoch: [60][0/391]\tTime 0.179 (0.179)\tData 0.133 (0.133)\tLoss 0.2125 (0.2125)\tPrec 90.625% (90.625%)\n",
      "Epoch: [60][100/391]\tTime 0.064 (0.055)\tData 0.003 (0.003)\tLoss 0.2728 (0.2412)\tPrec 89.844% (91.399%)\n",
      "Epoch: [60][200/391]\tTime 0.053 (0.053)\tData 0.003 (0.003)\tLoss 0.2250 (0.2374)\tPrec 92.188% (91.546%)\n",
      "Epoch: [60][300/391]\tTime 0.052 (0.052)\tData 0.002 (0.003)\tLoss 0.1522 (0.2405)\tPrec 95.312% (91.414%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.3854 (0.3854)\tPrec 84.375% (84.375%)\n",
      " * Prec 86.140% \n",
      "best acc: 87.230000\n",
      "Epoch: [61][0/391]\tTime 0.188 (0.188)\tData 0.123 (0.123)\tLoss 0.3644 (0.3644)\tPrec 85.938% (85.938%)\n",
      "Epoch: [61][100/391]\tTime 0.055 (0.060)\tData 0.002 (0.004)\tLoss 0.2153 (0.2222)\tPrec 91.406% (92.095%)\n",
      "Epoch: [61][200/391]\tTime 0.057 (0.059)\tData 0.002 (0.003)\tLoss 0.2830 (0.2387)\tPrec 92.188% (91.620%)\n",
      "Epoch: [61][300/391]\tTime 0.050 (0.058)\tData 0.002 (0.003)\tLoss 0.2711 (0.2387)\tPrec 91.406% (91.596%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.137 (0.137)\tLoss 0.2250 (0.2250)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.600% \n",
      "best acc: 87.230000\n",
      "Epoch: [62][0/391]\tTime 0.193 (0.193)\tData 0.130 (0.130)\tLoss 0.1982 (0.1982)\tPrec 92.188% (92.188%)\n",
      "Epoch: [62][100/391]\tTime 0.056 (0.060)\tData 0.002 (0.004)\tLoss 0.2208 (0.2433)\tPrec 89.062% (91.143%)\n",
      "Epoch: [62][200/391]\tTime 0.050 (0.060)\tData 0.002 (0.003)\tLoss 0.2842 (0.2445)\tPrec 89.844% (91.375%)\n",
      "Epoch: [62][300/391]\tTime 0.056 (0.058)\tData 0.002 (0.003)\tLoss 0.2762 (0.2461)\tPrec 88.281% (91.289%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.4073 (0.4073)\tPrec 85.156% (85.156%)\n",
      " * Prec 86.550% \n",
      "best acc: 87.230000\n",
      "Epoch: [63][0/391]\tTime 0.178 (0.178)\tData 0.121 (0.121)\tLoss 0.2073 (0.2073)\tPrec 92.188% (92.188%)\n",
      "Epoch: [63][100/391]\tTime 0.064 (0.062)\tData 0.002 (0.004)\tLoss 0.2584 (0.2270)\tPrec 90.625% (92.087%)\n",
      "Epoch: [63][200/391]\tTime 0.051 (0.058)\tData 0.003 (0.003)\tLoss 0.1371 (0.2346)\tPrec 94.531% (91.639%)\n",
      "Epoch: [63][300/391]\tTime 0.054 (0.058)\tData 0.003 (0.003)\tLoss 0.2986 (0.2403)\tPrec 89.062% (91.450%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.3237 (0.3237)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.210% \n",
      "best acc: 87.230000\n",
      "Epoch: [64][0/391]\tTime 0.193 (0.193)\tData 0.124 (0.124)\tLoss 0.1825 (0.1825)\tPrec 89.844% (89.844%)\n",
      "Epoch: [64][100/391]\tTime 0.055 (0.059)\tData 0.002 (0.003)\tLoss 0.3007 (0.2383)\tPrec 88.281% (91.484%)\n",
      "Epoch: [64][200/391]\tTime 0.063 (0.059)\tData 0.003 (0.003)\tLoss 0.2455 (0.2358)\tPrec 89.062% (91.639%)\n",
      "Epoch: [64][300/391]\tTime 0.054 (0.060)\tData 0.002 (0.003)\tLoss 0.2121 (0.2401)\tPrec 89.844% (91.502%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.4680 (0.4680)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.510% \n",
      "best acc: 87.230000\n",
      "Epoch: [65][0/391]\tTime 0.179 (0.179)\tData 0.122 (0.122)\tLoss 0.2853 (0.2853)\tPrec 89.062% (89.062%)\n",
      "Epoch: [65][100/391]\tTime 0.044 (0.050)\tData 0.002 (0.003)\tLoss 0.2731 (0.2463)\tPrec 89.844% (91.313%)\n",
      "Epoch: [65][200/391]\tTime 0.046 (0.051)\tData 0.002 (0.003)\tLoss 0.1952 (0.2452)\tPrec 93.750% (91.375%)\n",
      "Epoch: [65][300/391]\tTime 0.051 (0.049)\tData 0.002 (0.002)\tLoss 0.2302 (0.2464)\tPrec 92.969% (91.230%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.2925 (0.2925)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.370% \n",
      "best acc: 87.230000\n",
      "Epoch: [66][0/391]\tTime 0.177 (0.177)\tData 0.118 (0.118)\tLoss 0.2087 (0.2087)\tPrec 93.750% (93.750%)\n",
      "Epoch: [66][100/391]\tTime 0.050 (0.053)\tData 0.002 (0.003)\tLoss 0.2430 (0.2323)\tPrec 91.406% (92.071%)\n",
      "Epoch: [66][200/391]\tTime 0.050 (0.053)\tData 0.002 (0.003)\tLoss 0.2853 (0.2418)\tPrec 89.062% (91.628%)\n",
      "Epoch: [66][300/391]\tTime 0.057 (0.055)\tData 0.003 (0.003)\tLoss 0.1949 (0.2449)\tPrec 93.750% (91.476%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.2828 (0.2828)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.380% \n",
      "best acc: 87.230000\n",
      "Epoch: [67][0/391]\tTime 0.156 (0.156)\tData 0.115 (0.115)\tLoss 0.2929 (0.2929)\tPrec 89.062% (89.062%)\n",
      "Epoch: [67][100/391]\tTime 0.049 (0.054)\tData 0.002 (0.003)\tLoss 0.3068 (0.2482)\tPrec 88.281% (91.275%)\n",
      "Epoch: [67][200/391]\tTime 0.056 (0.055)\tData 0.003 (0.003)\tLoss 0.2049 (0.2479)\tPrec 96.094% (91.251%)\n",
      "Epoch: [67][300/391]\tTime 0.062 (0.056)\tData 0.003 (0.003)\tLoss 0.1623 (0.2482)\tPrec 94.531% (91.212%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.3570 (0.3570)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.820% \n",
      "best acc: 87.230000\n",
      "Epoch: [68][0/391]\tTime 0.184 (0.184)\tData 0.128 (0.128)\tLoss 0.2243 (0.2243)\tPrec 92.969% (92.969%)\n",
      "Epoch: [68][100/391]\tTime 0.065 (0.066)\tData 0.002 (0.006)\tLoss 0.1775 (0.2313)\tPrec 92.188% (91.654%)\n",
      "Epoch: [68][200/391]\tTime 0.042 (0.061)\tData 0.003 (0.004)\tLoss 0.2756 (0.2354)\tPrec 89.062% (91.620%)\n",
      "Epoch: [68][300/391]\tTime 0.054 (0.055)\tData 0.002 (0.004)\tLoss 0.2250 (0.2396)\tPrec 91.406% (91.489%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.137 (0.137)\tLoss 0.3708 (0.3708)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.530% \n",
      "best acc: 87.230000\n",
      "Epoch: [69][0/391]\tTime 0.180 (0.180)\tData 0.123 (0.123)\tLoss 0.3769 (0.3769)\tPrec 89.062% (89.062%)\n",
      "Epoch: [69][100/391]\tTime 0.055 (0.051)\tData 0.002 (0.003)\tLoss 0.2622 (0.2238)\tPrec 90.625% (92.218%)\n",
      "Epoch: [69][200/391]\tTime 0.058 (0.054)\tData 0.002 (0.003)\tLoss 0.2745 (0.2367)\tPrec 90.625% (91.604%)\n",
      "Epoch: [69][300/391]\tTime 0.053 (0.055)\tData 0.002 (0.003)\tLoss 0.2701 (0.2389)\tPrec 90.625% (91.492%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.2604 (0.2604)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.080% \n",
      "best acc: 87.230000\n",
      "Epoch: [70][0/391]\tTime 0.187 (0.187)\tData 0.135 (0.135)\tLoss 0.2307 (0.2307)\tPrec 91.406% (91.406%)\n",
      "Epoch: [70][100/391]\tTime 0.059 (0.053)\tData 0.002 (0.003)\tLoss 0.1858 (0.2363)\tPrec 92.188% (91.816%)\n",
      "Epoch: [70][200/391]\tTime 0.045 (0.052)\tData 0.002 (0.003)\tLoss 0.2672 (0.2414)\tPrec 89.844% (91.569%)\n",
      "Epoch: [70][300/391]\tTime 0.051 (0.053)\tData 0.002 (0.003)\tLoss 0.2166 (0.2408)\tPrec 93.750% (91.593%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.132 (0.132)\tLoss 0.2995 (0.2995)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.600% \n",
      "best acc: 87.230000\n",
      "Epoch: [71][0/391]\tTime 0.184 (0.184)\tData 0.131 (0.131)\tLoss 0.2485 (0.2485)\tPrec 90.625% (90.625%)\n",
      "Epoch: [71][100/391]\tTime 0.055 (0.057)\tData 0.002 (0.004)\tLoss 0.2534 (0.2441)\tPrec 89.844% (91.685%)\n",
      "Epoch: [71][200/391]\tTime 0.054 (0.060)\tData 0.002 (0.003)\tLoss 0.2261 (0.2396)\tPrec 92.188% (91.636%)\n",
      "Epoch: [71][300/391]\tTime 0.050 (0.056)\tData 0.002 (0.003)\tLoss 0.1772 (0.2429)\tPrec 92.188% (91.479%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.3346 (0.3346)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.170% \n",
      "best acc: 87.230000\n",
      "Epoch: [72][0/391]\tTime 0.205 (0.205)\tData 0.142 (0.142)\tLoss 0.2317 (0.2317)\tPrec 89.844% (89.844%)\n",
      "Epoch: [72][100/391]\tTime 0.046 (0.060)\tData 0.002 (0.004)\tLoss 0.2405 (0.2347)\tPrec 91.406% (91.553%)\n",
      "Epoch: [72][200/391]\tTime 0.060 (0.058)\tData 0.002 (0.003)\tLoss 0.2166 (0.2353)\tPrec 89.844% (91.538%)\n",
      "Epoch: [72][300/391]\tTime 0.068 (0.060)\tData 0.002 (0.003)\tLoss 0.2773 (0.2366)\tPrec 88.281% (91.546%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 0.2117 (0.2117)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.400% \n",
      "best acc: 87.230000\n",
      "Epoch: [73][0/391]\tTime 0.177 (0.177)\tData 0.127 (0.127)\tLoss 0.2679 (0.2679)\tPrec 91.406% (91.406%)\n",
      "Epoch: [73][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.003)\tLoss 0.1909 (0.2326)\tPrec 95.312% (91.785%)\n",
      "Epoch: [73][200/391]\tTime 0.042 (0.049)\tData 0.002 (0.003)\tLoss 0.3284 (0.2389)\tPrec 90.625% (91.566%)\n",
      "Epoch: [73][300/391]\tTime 0.065 (0.051)\tData 0.003 (0.003)\tLoss 0.3183 (0.2408)\tPrec 85.938% (91.487%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.125 (0.125)\tLoss 0.2995 (0.2995)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.480% \n",
      "best acc: 87.230000\n",
      "Epoch: [74][0/391]\tTime 0.193 (0.193)\tData 0.126 (0.126)\tLoss 0.1691 (0.1691)\tPrec 93.750% (93.750%)\n",
      "Epoch: [74][100/391]\tTime 0.067 (0.060)\tData 0.003 (0.004)\tLoss 0.2591 (0.2429)\tPrec 89.062% (91.282%)\n",
      "Epoch: [74][200/391]\tTime 0.059 (0.062)\tData 0.003 (0.003)\tLoss 0.1381 (0.2457)\tPrec 95.312% (91.227%)\n",
      "Epoch: [74][300/391]\tTime 0.065 (0.062)\tData 0.003 (0.003)\tLoss 0.2348 (0.2455)\tPrec 90.625% (91.284%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.137 (0.137)\tLoss 0.4643 (0.4643)\tPrec 82.812% (82.812%)\n",
      " * Prec 84.100% \n",
      "best acc: 87.230000\n",
      "Epoch: [75][0/391]\tTime 0.174 (0.174)\tData 0.115 (0.115)\tLoss 0.2791 (0.2791)\tPrec 93.750% (93.750%)\n",
      "Epoch: [75][100/391]\tTime 0.064 (0.058)\tData 0.003 (0.003)\tLoss 0.1970 (0.2444)\tPrec 92.969% (91.406%)\n",
      "Epoch: [75][200/391]\tTime 0.047 (0.057)\tData 0.002 (0.003)\tLoss 0.1517 (0.2422)\tPrec 95.312% (91.453%)\n",
      "Epoch: [75][300/391]\tTime 0.055 (0.054)\tData 0.002 (0.003)\tLoss 0.1842 (0.2437)\tPrec 92.188% (91.302%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.3484 (0.3484)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.820% \n",
      "best acc: 87.230000\n",
      "Epoch: [76][0/391]\tTime 0.183 (0.183)\tData 0.136 (0.136)\tLoss 0.2124 (0.2124)\tPrec 92.188% (92.188%)\n",
      "Epoch: [76][100/391]\tTime 0.058 (0.059)\tData 0.002 (0.004)\tLoss 0.3052 (0.2319)\tPrec 86.719% (91.723%)\n",
      "Epoch: [76][200/391]\tTime 0.058 (0.061)\tData 0.002 (0.003)\tLoss 0.1823 (0.2380)\tPrec 92.969% (91.612%)\n",
      "Epoch: [76][300/391]\tTime 0.065 (0.061)\tData 0.003 (0.003)\tLoss 0.2904 (0.2391)\tPrec 92.188% (91.642%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.134 (0.134)\tLoss 0.3119 (0.3119)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.300% \n",
      "best acc: 87.230000\n",
      "Epoch: [77][0/391]\tTime 0.172 (0.172)\tData 0.122 (0.122)\tLoss 0.2517 (0.2517)\tPrec 92.969% (92.969%)\n",
      "Epoch: [77][100/391]\tTime 0.064 (0.056)\tData 0.002 (0.003)\tLoss 0.2669 (0.2292)\tPrec 92.969% (91.754%)\n",
      "Epoch: [77][200/391]\tTime 0.058 (0.058)\tData 0.003 (0.003)\tLoss 0.2998 (0.2340)\tPrec 87.500% (91.620%)\n",
      "Epoch: [77][300/391]\tTime 0.064 (0.059)\tData 0.003 (0.003)\tLoss 0.2968 (0.2364)\tPrec 85.938% (91.552%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.2742 (0.2742)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.510% \n",
      "best acc: 87.230000\n",
      "Epoch: [78][0/391]\tTime 0.185 (0.185)\tData 0.122 (0.122)\tLoss 0.2051 (0.2051)\tPrec 94.531% (94.531%)\n",
      "Epoch: [78][100/391]\tTime 0.048 (0.058)\tData 0.002 (0.004)\tLoss 0.2067 (0.2257)\tPrec 95.312% (92.133%)\n",
      "Epoch: [78][200/391]\tTime 0.063 (0.059)\tData 0.002 (0.003)\tLoss 0.1866 (0.2348)\tPrec 95.312% (91.853%)\n",
      "Epoch: [78][300/391]\tTime 0.064 (0.060)\tData 0.003 (0.003)\tLoss 0.2248 (0.2388)\tPrec 92.969% (91.655%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 0.3205 (0.3205)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.940% \n",
      "best acc: 87.230000\n",
      "Epoch: [79][0/391]\tTime 0.188 (0.188)\tData 0.129 (0.129)\tLoss 0.2590 (0.2590)\tPrec 91.406% (91.406%)\n",
      "Epoch: [79][100/391]\tTime 0.056 (0.060)\tData 0.002 (0.004)\tLoss 0.2422 (0.2320)\tPrec 89.062% (91.886%)\n",
      "Epoch: [79][200/391]\tTime 0.053 (0.060)\tData 0.002 (0.003)\tLoss 0.1815 (0.2416)\tPrec 93.750% (91.632%)\n",
      "Epoch: [79][300/391]\tTime 0.062 (0.060)\tData 0.002 (0.003)\tLoss 0.2646 (0.2436)\tPrec 91.406% (91.484%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.131 (0.131)\tLoss 0.3239 (0.3239)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.900% \n",
      "best acc: 87.230000\n",
      "Epoch: [80][0/391]\tTime 0.166 (0.166)\tData 0.119 (0.119)\tLoss 0.2969 (0.2969)\tPrec 91.406% (91.406%)\n",
      "Epoch: [80][100/391]\tTime 0.062 (0.060)\tData 0.002 (0.003)\tLoss 0.2788 (0.2145)\tPrec 89.062% (92.358%)\n",
      "Epoch: [80][200/391]\tTime 0.062 (0.060)\tData 0.002 (0.003)\tLoss 0.2780 (0.2237)\tPrec 89.062% (92.048%)\n",
      "Epoch: [80][300/391]\tTime 0.061 (0.060)\tData 0.002 (0.003)\tLoss 0.2969 (0.2335)\tPrec 89.062% (91.655%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.4275 (0.4275)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.620% \n",
      "best acc: 87.230000\n",
      "Epoch: [81][0/391]\tTime 0.184 (0.184)\tData 0.130 (0.130)\tLoss 0.2660 (0.2660)\tPrec 92.188% (92.188%)\n",
      "Epoch: [81][100/391]\tTime 0.064 (0.061)\tData 0.002 (0.004)\tLoss 0.2574 (0.2337)\tPrec 89.844% (91.832%)\n",
      "Epoch: [81][200/391]\tTime 0.064 (0.061)\tData 0.003 (0.003)\tLoss 0.1553 (0.2321)\tPrec 96.094% (91.880%)\n",
      "Epoch: [81][300/391]\tTime 0.064 (0.062)\tData 0.002 (0.003)\tLoss 0.1714 (0.2343)\tPrec 92.969% (91.694%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.131 (0.131)\tLoss 0.3365 (0.3365)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.460% \n",
      "best acc: 87.230000\n",
      "Epoch: [82][0/391]\tTime 0.175 (0.175)\tData 0.115 (0.115)\tLoss 0.2226 (0.2226)\tPrec 93.750% (93.750%)\n",
      "Epoch: [82][100/391]\tTime 0.043 (0.060)\tData 0.002 (0.003)\tLoss 0.2402 (0.2412)\tPrec 94.531% (91.313%)\n",
      "Epoch: [82][200/391]\tTime 0.063 (0.058)\tData 0.002 (0.003)\tLoss 0.2065 (0.2450)\tPrec 92.188% (91.204%)\n",
      "Epoch: [82][300/391]\tTime 0.064 (0.060)\tData 0.003 (0.003)\tLoss 0.1716 (0.2431)\tPrec 93.750% (91.292%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.3649 (0.3649)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.720% \n",
      "best acc: 87.230000\n",
      "Epoch: [83][0/391]\tTime 0.175 (0.175)\tData 0.115 (0.115)\tLoss 0.3068 (0.3068)\tPrec 91.406% (91.406%)\n",
      "Epoch: [83][100/391]\tTime 0.057 (0.055)\tData 0.003 (0.003)\tLoss 0.2505 (0.2372)\tPrec 91.406% (91.484%)\n",
      "Epoch: [83][200/391]\tTime 0.068 (0.056)\tData 0.003 (0.003)\tLoss 0.4157 (0.2445)\tPrec 85.156% (91.243%)\n",
      "Epoch: [83][300/391]\tTime 0.056 (0.057)\tData 0.002 (0.003)\tLoss 0.1481 (0.2415)\tPrec 92.969% (91.375%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.3329 (0.3329)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.620% \n",
      "best acc: 87.230000\n",
      "Epoch: [84][0/391]\tTime 0.175 (0.175)\tData 0.115 (0.115)\tLoss 0.1722 (0.1722)\tPrec 94.531% (94.531%)\n",
      "Epoch: [84][100/391]\tTime 0.065 (0.060)\tData 0.002 (0.003)\tLoss 0.3090 (0.2372)\tPrec 87.500% (91.484%)\n",
      "Epoch: [84][200/391]\tTime 0.054 (0.057)\tData 0.002 (0.003)\tLoss 0.3399 (0.2407)\tPrec 85.938% (91.379%)\n",
      "Epoch: [84][300/391]\tTime 0.066 (0.059)\tData 0.002 (0.003)\tLoss 0.3127 (0.2418)\tPrec 86.719% (91.347%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.3678 (0.3678)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.650% \n",
      "best acc: 87.230000\n",
      "Epoch: [85][0/391]\tTime 0.181 (0.181)\tData 0.119 (0.119)\tLoss 0.2114 (0.2114)\tPrec 92.969% (92.969%)\n",
      "Epoch: [85][100/391]\tTime 0.062 (0.056)\tData 0.002 (0.003)\tLoss 0.2083 (0.2264)\tPrec 95.312% (92.048%)\n",
      "Epoch: [85][200/391]\tTime 0.062 (0.058)\tData 0.002 (0.003)\tLoss 0.2449 (0.2359)\tPrec 89.844% (91.752%)\n",
      "Epoch: [85][300/391]\tTime 0.064 (0.059)\tData 0.002 (0.003)\tLoss 0.3153 (0.2404)\tPrec 89.062% (91.544%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.3434 (0.3434)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.610% \n",
      "best acc: 87.230000\n",
      "Epoch: [86][0/391]\tTime 0.192 (0.192)\tData 0.144 (0.144)\tLoss 0.2359 (0.2359)\tPrec 90.625% (90.625%)\n",
      "Epoch: [86][100/391]\tTime 0.058 (0.062)\tData 0.002 (0.004)\tLoss 0.2682 (0.2249)\tPrec 91.406% (91.925%)\n",
      "Epoch: [86][200/391]\tTime 0.063 (0.060)\tData 0.002 (0.003)\tLoss 0.1322 (0.2328)\tPrec 94.531% (91.628%)\n",
      "Epoch: [86][300/391]\tTime 0.064 (0.060)\tData 0.002 (0.003)\tLoss 0.2218 (0.2361)\tPrec 94.531% (91.518%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.2506 (0.2506)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.420% \n",
      "best acc: 87.420000\n",
      "Epoch: [87][0/391]\tTime 0.184 (0.184)\tData 0.131 (0.131)\tLoss 0.1355 (0.1355)\tPrec 92.969% (92.969%)\n",
      "Epoch: [87][100/391]\tTime 0.060 (0.061)\tData 0.002 (0.004)\tLoss 0.2890 (0.2384)\tPrec 88.281% (91.460%)\n",
      "Epoch: [87][200/391]\tTime 0.063 (0.060)\tData 0.002 (0.003)\tLoss 0.2795 (0.2431)\tPrec 89.844% (91.301%)\n",
      "Epoch: [87][300/391]\tTime 0.062 (0.060)\tData 0.002 (0.003)\tLoss 0.1845 (0.2444)\tPrec 92.188% (91.245%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.3439 (0.3439)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.000% \n",
      "best acc: 87.420000\n",
      "Epoch: [88][0/391]\tTime 0.192 (0.192)\tData 0.126 (0.126)\tLoss 0.1679 (0.1679)\tPrec 94.531% (94.531%)\n",
      "Epoch: [88][100/391]\tTime 0.063 (0.062)\tData 0.002 (0.004)\tLoss 0.2220 (0.2332)\tPrec 95.312% (91.723%)\n",
      "Epoch: [88][200/391]\tTime 0.059 (0.062)\tData 0.002 (0.003)\tLoss 0.1765 (0.2434)\tPrec 92.188% (91.414%)\n",
      "Epoch: [88][300/391]\tTime 0.062 (0.061)\tData 0.002 (0.003)\tLoss 0.2293 (0.2442)\tPrec 91.406% (91.302%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.2494 (0.2494)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.550% \n",
      "best acc: 87.420000\n",
      "Epoch: [89][0/391]\tTime 0.182 (0.182)\tData 0.118 (0.118)\tLoss 0.3396 (0.3396)\tPrec 82.812% (82.812%)\n",
      "Epoch: [89][100/391]\tTime 0.062 (0.058)\tData 0.002 (0.003)\tLoss 0.3173 (0.2339)\tPrec 86.719% (91.569%)\n",
      "Epoch: [89][200/391]\tTime 0.056 (0.059)\tData 0.002 (0.003)\tLoss 0.2646 (0.2371)\tPrec 90.625% (91.496%)\n",
      "Epoch: [89][300/391]\tTime 0.063 (0.060)\tData 0.002 (0.003)\tLoss 0.3026 (0.2394)\tPrec 89.844% (91.398%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.132 (0.132)\tLoss 0.3580 (0.3580)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.480% \n",
      "best acc: 87.420000\n",
      "Epoch: [90][0/391]\tTime 0.182 (0.182)\tData 0.118 (0.118)\tLoss 0.2325 (0.2325)\tPrec 92.969% (92.969%)\n",
      "Epoch: [90][100/391]\tTime 0.053 (0.059)\tData 0.002 (0.003)\tLoss 0.1915 (0.2363)\tPrec 94.531% (91.468%)\n",
      "Epoch: [90][200/391]\tTime 0.055 (0.060)\tData 0.003 (0.003)\tLoss 0.2406 (0.2370)\tPrec 93.750% (91.445%)\n",
      "Epoch: [90][300/391]\tTime 0.062 (0.060)\tData 0.002 (0.003)\tLoss 0.3068 (0.2394)\tPrec 86.719% (91.380%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.127 (0.127)\tLoss 0.3648 (0.3648)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.590% \n",
      "best acc: 87.420000\n",
      "Epoch: [91][0/391]\tTime 0.173 (0.173)\tData 0.125 (0.125)\tLoss 0.3619 (0.3619)\tPrec 85.938% (85.938%)\n",
      "Epoch: [91][100/391]\tTime 0.061 (0.057)\tData 0.002 (0.003)\tLoss 0.2301 (0.2354)\tPrec 90.625% (91.406%)\n",
      "Epoch: [91][200/391]\tTime 0.062 (0.059)\tData 0.002 (0.003)\tLoss 0.2289 (0.2374)\tPrec 89.062% (91.484%)\n",
      "Epoch: [91][300/391]\tTime 0.049 (0.058)\tData 0.001 (0.003)\tLoss 0.3664 (0.2415)\tPrec 86.719% (91.344%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.3658 (0.3658)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.840% \n",
      "best acc: 87.420000\n",
      "Epoch: [92][0/391]\tTime 0.178 (0.178)\tData 0.126 (0.126)\tLoss 0.2511 (0.2511)\tPrec 89.844% (89.844%)\n",
      "Epoch: [92][100/391]\tTime 0.057 (0.055)\tData 0.002 (0.003)\tLoss 0.2626 (0.2368)\tPrec 90.625% (91.576%)\n",
      "Epoch: [92][200/391]\tTime 0.057 (0.057)\tData 0.002 (0.003)\tLoss 0.2400 (0.2354)\tPrec 93.750% (91.535%)\n",
      "Epoch: [92][300/391]\tTime 0.062 (0.058)\tData 0.002 (0.003)\tLoss 0.2818 (0.2362)\tPrec 90.625% (91.572%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.4495 (0.4495)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.490% \n",
      "best acc: 87.420000\n",
      "Epoch: [93][0/391]\tTime 0.176 (0.176)\tData 0.119 (0.119)\tLoss 0.2619 (0.2619)\tPrec 89.062% (89.062%)\n",
      "Epoch: [93][100/391]\tTime 0.058 (0.053)\tData 0.002 (0.003)\tLoss 0.1814 (0.2337)\tPrec 94.531% (91.700%)\n",
      "Epoch: [93][200/391]\tTime 0.063 (0.056)\tData 0.003 (0.003)\tLoss 0.3226 (0.2427)\tPrec 87.500% (91.383%)\n",
      "Epoch: [93][300/391]\tTime 0.059 (0.057)\tData 0.002 (0.003)\tLoss 0.2124 (0.2420)\tPrec 92.188% (91.396%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.2681 (0.2681)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.700% \n",
      "best acc: 87.420000\n",
      "Epoch: [94][0/391]\tTime 0.189 (0.189)\tData 0.124 (0.124)\tLoss 0.2585 (0.2585)\tPrec 90.625% (90.625%)\n",
      "Epoch: [94][100/391]\tTime 0.065 (0.064)\tData 0.003 (0.004)\tLoss 0.3186 (0.2262)\tPrec 89.844% (92.249%)\n",
      "Epoch: [94][200/391]\tTime 0.057 (0.060)\tData 0.002 (0.003)\tLoss 0.3200 (0.2340)\tPrec 85.938% (91.647%)\n",
      "Epoch: [94][300/391]\tTime 0.062 (0.060)\tData 0.003 (0.003)\tLoss 0.2854 (0.2382)\tPrec 90.625% (91.500%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.132 (0.132)\tLoss 0.3416 (0.3416)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.010% \n",
      "best acc: 87.420000\n",
      "Epoch: [95][0/391]\tTime 0.182 (0.182)\tData 0.129 (0.129)\tLoss 0.2038 (0.2038)\tPrec 93.750% (93.750%)\n",
      "Epoch: [95][100/391]\tTime 0.050 (0.063)\tData 0.002 (0.004)\tLoss 0.1674 (0.2263)\tPrec 92.188% (91.955%)\n",
      "Epoch: [95][200/391]\tTime 0.058 (0.060)\tData 0.002 (0.003)\tLoss 0.2096 (0.2287)\tPrec 92.969% (91.849%)\n",
      "Epoch: [95][300/391]\tTime 0.047 (0.056)\tData 0.002 (0.003)\tLoss 0.3335 (0.2376)\tPrec 85.938% (91.554%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.3098 (0.3098)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.770% \n",
      "best acc: 87.420000\n",
      "Epoch: [96][0/391]\tTime 0.182 (0.182)\tData 0.130 (0.130)\tLoss 0.2122 (0.2122)\tPrec 92.188% (92.188%)\n",
      "Epoch: [96][100/391]\tTime 0.055 (0.061)\tData 0.002 (0.003)\tLoss 0.2524 (0.2370)\tPrec 90.625% (91.778%)\n",
      "Epoch: [96][200/391]\tTime 0.057 (0.061)\tData 0.003 (0.003)\tLoss 0.3366 (0.2375)\tPrec 88.281% (91.698%)\n",
      "Epoch: [96][300/391]\tTime 0.066 (0.062)\tData 0.003 (0.003)\tLoss 0.2962 (0.2404)\tPrec 89.844% (91.583%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.3267 (0.3267)\tPrec 85.156% (85.156%)\n",
      " * Prec 87.590% \n",
      "best acc: 87.590000\n",
      "Epoch: [97][0/391]\tTime 0.204 (0.204)\tData 0.144 (0.144)\tLoss 0.1476 (0.1476)\tPrec 96.094% (96.094%)\n",
      "Epoch: [97][100/391]\tTime 0.057 (0.060)\tData 0.002 (0.004)\tLoss 0.3552 (0.2372)\tPrec 86.719% (91.383%)\n",
      "Epoch: [97][200/391]\tTime 0.066 (0.059)\tData 0.002 (0.003)\tLoss 0.3892 (0.2412)\tPrec 84.375% (91.472%)\n",
      "Epoch: [97][300/391]\tTime 0.045 (0.059)\tData 0.002 (0.003)\tLoss 0.1854 (0.2368)\tPrec 93.750% (91.759%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.3506 (0.3506)\tPrec 85.156% (85.156%)\n",
      " * Prec 86.590% \n",
      "best acc: 87.590000\n",
      "Epoch: [98][0/391]\tTime 0.181 (0.181)\tData 0.129 (0.129)\tLoss 0.2773 (0.2773)\tPrec 89.062% (89.062%)\n",
      "Epoch: [98][100/391]\tTime 0.062 (0.061)\tData 0.002 (0.003)\tLoss 0.3353 (0.2424)\tPrec 90.625% (91.282%)\n",
      "Epoch: [98][200/391]\tTime 0.060 (0.061)\tData 0.002 (0.003)\tLoss 0.1713 (0.2368)\tPrec 92.969% (91.515%)\n",
      "Epoch: [98][300/391]\tTime 0.062 (0.061)\tData 0.003 (0.003)\tLoss 0.3184 (0.2394)\tPrec 86.719% (91.510%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.2755 (0.2755)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.300% \n",
      "best acc: 87.590000\n",
      "Epoch: [99][0/391]\tTime 0.169 (0.169)\tData 0.119 (0.119)\tLoss 0.1683 (0.1683)\tPrec 93.750% (93.750%)\n",
      "Epoch: [99][100/391]\tTime 0.055 (0.059)\tData 0.002 (0.003)\tLoss 0.2187 (0.2207)\tPrec 93.750% (92.280%)\n",
      "Epoch: [99][200/391]\tTime 0.062 (0.058)\tData 0.002 (0.003)\tLoss 0.3328 (0.2289)\tPrec 89.062% (91.966%)\n",
      "Epoch: [99][300/391]\tTime 0.064 (0.058)\tData 0.002 (0.003)\tLoss 0.2884 (0.2315)\tPrec 91.406% (91.860%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.3921 (0.3921)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.800% \n",
      "best acc: 87.590000\n"
     ]
    }
   ],
   "source": [
    "## Start finetuning (training here), and see how much you can recover your accuracy ##\n",
    "## You can change hyper parameters such as epochs or lr ##\n",
    "lr = 5e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "# weight decay: for regularization to prevent overfitting\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "    \n",
    "fdir = 'result/'+str(model_name)+str('_unstructured')\n",
    "\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "064c8adc-ae0a-4fd2-917b-044b7abf0849",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8759/10000 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check your accuracy again after finetuning\n",
    "PATH = \"result/resnet20_quant_unstructured/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78f8a7e7-93e3-45b3-a758-e1254d4c1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        #print(layer,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "spoken-worst",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000, -4.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-2.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -2.0000, -2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-7.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -3.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -2.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  3.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  2.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -2.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-2.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -2.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-2.0000, -2.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-2.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-3.0000,  0.0000,  0.0000],\n",
      "          [-1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-7.0000,  7.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0000, -1.0000, -7.0000],\n",
      "          [ 5.0000,  0.0000, -4.0000],\n",
      "          [ 0.0000,  1.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000, -1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.0000, -0.0000, -3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -2.0000,  0.0000],\n",
      "          [-3.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.layer1[2].conv1.weight_q                                      # quantized value is stored during the training\n",
    "w_alpha = model.layer1[2].conv1.weight_quant.wgt_alpha.data.item()             # alpha is defined in your model already. bring it out here\n",
    "w_delta = w_alpha / (2**(w_bit-1)-1)                                           # delta can be calculated by using alpha and w_bit\n",
    "weight_int = weight_q / w_delta                                                # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int)                                                              # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "interior-oxygen",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "act = save_output.outputs[4][0]\n",
    "act_alpha  = model.layer1[2].conv1.act_alpha\n",
    "act_bit = 4\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "act_q = act_quant_fn(act, act_alpha)\n",
    "\n",
    "act_int = act_q / (act_alpha / (2**act_bit-1))\n",
    "#print(act_int)                         # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ranging-porter",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This cell is provided\n",
    "\n",
    "conv_int = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, padding=1, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "#conv_int.bias = model.features[27].bias\n",
    "output_int = conv_int(act_int)\n",
    "output_recovered = output_int * (act_alpha / (2**act_bit-1)) * (w_alpha / (2**(w_bit-1)-1))\n",
    "#print(output_recovered)\n",
    "bn1 = model.layer1[2].bn1\n",
    "relu = model.layer1[2].relu\n",
    "output_recovered = relu(bn1(output_recovered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "482fbbe1-6f20-4c9c-af03-53a1140db7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSUM recovered error:\n",
      "4.3064773080914165e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"PSUM recovered error:\")\n",
    "print(abs((save_output.outputs[5][0] - output_recovered)).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "157dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2595, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "                         ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf8062b4-b528-4a8e-89d3-e1df30f0fd10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# act_int.size = torch.Size([128, 16, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = act_int[0,:,0:6,0:6]  # pick only one input out of batch\n",
    "# a_int.size() = [16, 6, 6]\n",
    "\n",
    "# conv_int.weight.size() = torch.Size([16, 16, 3, 3])  <- output_ch, input_ch, ki, kj\n",
    "\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    "# w_int.weight.size() = torch.Size([16, 16, 9])\n",
    "                      \n",
    "padding = 0\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group [0,1,...5]\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "nijg = range(a_int.size(1) * a_int.size(2))\n",
    "\n",
    "icg = range(int(w_int.size(1)))  ## input channel [0,...16]\n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "ic_tileg = range(int(w_int.size(1) / array_size)) ## [0,1]\n",
    "oc_tileg = range(int(w_int.size(0) / array_size)) ## [0,1]\n",
    "\n",
    "kijg = range(w_int.size(2)) # [0, .. 8]\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size, 3\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(njg)+padding*2).cuda()\n",
    "# a_pad.size() = [16, 6+0pad, 6+0pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))  ## mergin ni and nj index into nij\n",
    "# a_pad.size() = [16, (6+0pad)*(6+0pad)]\n",
    "\n",
    "a_pad_temp = torch.reshape(a_pad, (len(ic_tileg), array_size, -1))\n",
    "### Input(a_tile) format: [ic_tile, oc_tile, ic index(row#), nij (time step)] ###\n",
    "a_tile = torch.tile(a_pad_temp[:, np.newaxis, :, :], (1, len(oc_tileg), 1, 1)) \n",
    "### Weight(w_tile) format: [ic_tile, oc_tile, ic index(row#), io index(col#), kij] ###\n",
    "w_tile = torch.transpose(torch.reshape(torch.stack([w_int[i:i+array_size, j:j+array_size] \\\n",
    "                                    for i in range(0, len(icg), array_size) \\\n",
    "                                    for j in range(0, len(ocg), array_size)]), \\\n",
    "                       (len(ic_tileg), len(oc_tileg), array_size, array_size, len(kijg))), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41dc4398-e86f-4e20-9321-5b3735a1959e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "p_nijg = range(a_tile.size(3)) ## paded activation's nij group [0, ...6*6-1]\n",
    "### psum(psum) format: [ic_tile, oc_tile, oc index(col#), nij(time step), kij] ###\n",
    "psum = torch.zeros(len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)).cuda() \n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    for oc_tile in oc_tileg:\n",
    "        for kij in kijg:       \n",
    "            for nij in p_nijg:     # time domain, sequentially given input\n",
    "                m = nn.Linear(array_size, array_size, bias=False)\n",
    "                m.weight = torch.nn.Parameter(w_tile[ic_tile, oc_tile, :, :, kij])\n",
    "                psum[ic_tile, oc_tile, :, nij, kij] = m(a_tile[ic_tile, oc_tile, :, nij]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a552ba7c-5751-472c-b5a8-b2b5bc0e75f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## Easier 2D version ########\n",
    "import math\n",
    "\n",
    "kig = range(int(math.sqrt(len(kijg))))\n",
    "kjg = range(int(math.sqrt(len(kijg))))\n",
    "    \n",
    "o_nig = range(int((math.sqrt(len(nijg))+2*padding-(math.sqrt(len(kijg))-1)-1)/stride+1))\n",
    "o_njg = range(int((math.sqrt(len(nijg))+2*padding-(math.sqrt(len(kijg))-1)-1)/stride+1))\n",
    "    \n",
    "out = torch.zeros(len(ocg), len(o_nig), len(o_njg)).cuda()\n",
    "### SFP accumulation ###\n",
    "for ni in o_nig:\n",
    "    for nj in o_njg:\n",
    "        for ki in kig:\n",
    "            for kj in kjg:\n",
    "                for ic_tile in ic_tileg:    \n",
    "                    for oc_tile in oc_tileg:   \n",
    "                        out[oc_tile*array_size:(oc_tile+1)*array_size,ni,nj] = out[oc_tile*array_size:(oc_tile+1)*array_size,ni,nj]+\\\n",
    "                        psum[ic_tile,oc_tile,:,int(math.sqrt(len(p_nijg)))*(ni+ki) + (nj+kj),len(kig)*ki+kj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa65ea44-98ab-43a9-991f-4d9ca9ad00f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0010, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = (out - output_int[0, :, 1:5, 1:5])\n",
    "print(difference.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64855859-b9b9-4dd1-a108-d18e5a32421f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output(out) format: [oc index(col#), o_nij] ###\n",
    "out = torch.reshape(out, (len(ocg), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab10064c-d8bf-4716-9e83-5bf4b5ce6b75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### show this cell partially. The following cells should be printed by students ###\n",
    "for p in range(a_tile.size(0)):\n",
    "    for q in range(a_tile.size(1)):\n",
    "        X = a_tile[p,q,:,:]  # [array row num, time_steps] only 36 values in an image at this layer\n",
    "        bit_precision = 4\n",
    "        file = open(f\"activation_{p}{q}.txt\", 'w') #write to file\n",
    "        file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "        file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "        file.write('#................#\\n')\n",
    "        for i in range(X.size(1)):  # time step\n",
    "            for j in range(X.size(0)): # row #\n",
    "                X_bin = '{0:04b}'.format(round(X[7-j,i].item()))\n",
    "                for k in range(bit_precision):\n",
    "                    file.write(X_bin[k])        \n",
    "                #file.write(' ')  # for visibility with blank between words, you can use\n",
    "            file.write('\\n')\n",
    "        file.close() #close file    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7e209a5-1bb3-4304-9895-ba6a831d41af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "for p in range(a_tile.size(0)):\n",
    "    for q in range(a_tile.size(1)):\n",
    "        bit_precision = 4\n",
    "        for k in range(w_int.size(2)):\n",
    "            W = w_tile[p,q,:,:,k]  # w_int[array col num, array row num, kij]\n",
    "            file = open(f\"weight_kij{k}_{p}{q}.txt\", 'w') #write to file\n",
    "            file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "            file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "            file.write('#................#\\n')\n",
    "            for i in range(W.size(0)):  # col #\n",
    "                for j in range(W.size(1)): # row #\n",
    "                    temp=round(W[i,7-j].item())\n",
    "                    if(temp < 0 ):\n",
    "                        temp=temp+16\n",
    "                    W_bin = '{0:04b}'.format(temp)\n",
    "                    for k in range(bit_precision):\n",
    "                        file.write(W_bin[k])        \n",
    "                    #file.write(' ')  # for visibility with blank between words, you can use\n",
    "                file.write('\\n')\n",
    "            file.close() #close file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1f3b9d5-feb9-4d12-a952-d35cab1cc8f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "for p in range(a_tile.size(0)):\n",
    "    for q in range(a_tile.size(1)):\n",
    "        bit_precision = 16\n",
    "        for k in range(psum.size(2)):\n",
    "            psum_temp = psum[p,q,:,:,k];\n",
    "            file = open(f\"psum_kij{k}_{p}{q}.txt\", 'w') #write to file\n",
    "            file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "            file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "            file.write('#................#\\n')\n",
    "            for i in range(psum_temp.size(1)):  # nijg #\n",
    "                for j in range(psum_temp.size(0)): # col #\n",
    "                    temp=round(psum_temp[7-j,i].item())\n",
    "                    if(temp < 0 ):\n",
    "                        temp=temp+65536\n",
    "                    W_bin = '{0:016b}'.format(temp)\n",
    "                    for b in range(bit_precision):\n",
    "                        file.write(W_bin[b])        \n",
    "                    #file.write(' ')  # for visibility with blank between words, you can use\n",
    "                file.write('\\n')\n",
    "            file.close() #close file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11661ae3-b4dd-4e86-9ab8-ccf71c48f6f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bit_precision = 16\n",
    "file = open('out_all.txt', 'w') #write to file\n",
    "file.write('#time0col15[msb-lsb],time0col14[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col15[msb-lsb],time1col14[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "#print(torch.reshape(output_int[0,:,:,:], (output_int[0,:,:,:].size(0), -1)).size())\n",
    "out_p = torch.relu(out)\n",
    "for i in range(out_p.size(1)):  # nijg #\n",
    "    for j in range(out_p.size(0)): # row #\n",
    "        temp=round(out_p[15-j,i].item())\n",
    "        if(temp < 0 ):\n",
    "            temp=temp+65536\n",
    "        W_bin = '{0:016b}'.format(temp)\n",
    "        for b in range(bit_precision):\n",
    "            file.write(W_bin[b])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574acc19-0902-4563-8e25-9e08ebceff78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
